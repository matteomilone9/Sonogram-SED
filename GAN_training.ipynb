{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43e95875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pydub import AudioSegment\n",
    "import pandas as pd\n",
    "from torchmetrics.audio import SignalDistortionRatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efa91a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, target_duration=10000, target_sample_rate=16000,\n",
    "                 target_channels=1):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.target_duration = target_duration\n",
    "        self.target_channels = target_channels\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        background_path = os.path.join(self.df.loc[idx, 'background'])\n",
    "        mixture_path = os.path.join(self.df.loc[idx, 'mixture'])\n",
    "\n",
    "        background_audio = AudioSegment.from_file(background_path).set_channels(self.target_channels).set_frame_rate(self.target_sample_rate)\n",
    "        mixture_audio = AudioSegment.from_file(mixture_path).set_channels(self.target_channels).set_frame_rate(self.target_sample_rate)\n",
    "\n",
    "        background, _ = self._pydub_to_array(background_audio)\n",
    "        mixture, _ = self._pydub_to_array(mixture_audio)\n",
    "\n",
    "        background_tensor = torch.Tensor(background)\n",
    "        mixture_tensor = torch.Tensor(mixture)\n",
    "\n",
    "        return mixture_tensor, background_tensor\n",
    "\n",
    "    def _pydub_to_array(self, audio: AudioSegment) -> (np.ndarray, int):\n",
    "        return np.array(audio.get_array_of_samples(), dtype=np.float32).reshape((audio.channels, -1)) / (\n",
    "                1 << (8 * audio.sample_width - 1)), audio.frame_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd618df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveUNetBlock(nn.Module):\n",
    "    \"\"\"Blocco base per Wave-U-Net con convoluzione 1D\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=15, stride=1, dilation=1):\n",
    "        super().__init__()\n",
    "        padding = (kernel_size - 1) // 2 * dilation\n",
    "        \n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, \n",
    "                             stride=stride, padding=padding, dilation=dilation)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.activation(self.bn(self.conv(x)))\n",
    "\n",
    "class WaveUNetGenerator(nn.Module):\n",
    "    \"\"\"Wave-U-Net Generator per separazione audio\"\"\"\n",
    "    def __init__(self, input_channels=2, output_channels=1, base_channels=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder (downsampling)\n",
    "        self.enc1 = WaveUNetBlock(input_channels, base_channels, kernel_size=15)\n",
    "        self.enc2 = WaveUNetBlock(base_channels, base_channels*2, stride=2)\n",
    "        self.enc3 = WaveUNetBlock(base_channels*2, base_channels*4, stride=2)\n",
    "        self.enc4 = WaveUNetBlock(base_channels*4, base_channels*8, stride=2)\n",
    "        self.enc5 = WaveUNetBlock(base_channels*8, base_channels*16, stride=2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = WaveUNetBlock(base_channels*16, base_channels*16, kernel_size=15)\n",
    "        \n",
    "        # Decoder (upsampling)\n",
    "        self.dec5 = nn.ConvTranspose1d(base_channels*16, base_channels*8, \n",
    "                                      kernel_size=4, stride=2, padding=1)\n",
    "        self.dec5_conv = WaveUNetBlock(base_channels*16, base_channels*8)\n",
    "        \n",
    "        self.dec4 = nn.ConvTranspose1d(base_channels*8, base_channels*4, \n",
    "                                      kernel_size=4, stride=2, padding=1)\n",
    "        self.dec4_conv = WaveUNetBlock(base_channels*8, base_channels*4)\n",
    "        \n",
    "        self.dec3 = nn.ConvTranspose1d(base_channels*4, base_channels*2, \n",
    "                                      kernel_size=4, stride=2, padding=1)\n",
    "        self.dec3_conv = WaveUNetBlock(base_channels*4, base_channels*2)\n",
    "        \n",
    "        self.dec2 = nn.ConvTranspose1d(base_channels*2, base_channels, \n",
    "                                      kernel_size=4, stride=2, padding=1)\n",
    "        self.dec2_conv = WaveUNetBlock(base_channels*2, base_channels)\n",
    "        \n",
    "        # Output layer\n",
    "        self.final_conv = nn.Conv1d(base_channels, output_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, mixture, source_b):\n",
    "        # Input: concatena mixture e source_b\n",
    "        x = torch.cat([mixture, source_b], dim=1)\n",
    "        \n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        e4 = self.enc4(e3)\n",
    "        e5 = self.enc5(e4)\n",
    "        \n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(e5)\n",
    "        \n",
    "        # Decoder con skip connections\n",
    "        d5 = self.dec5(b)\n",
    "        d5 = torch.cat([d5, e4], dim=1)\n",
    "        d5 = self.dec5_conv(d5)\n",
    "        \n",
    "        d4 = self.dec4(d5)\n",
    "        d4 = torch.cat([d4, e3], dim=1)\n",
    "        d4 = self.dec4_conv(d4)\n",
    "        \n",
    "        d3 = self.dec3(d4)\n",
    "        d3 = torch.cat([d3, e2], dim=1)\n",
    "        d3 = self.dec3_conv(d3)\n",
    "        \n",
    "        d2 = self.dec2(d3)\n",
    "        d2 = torch.cat([d2, e1], dim=1)\n",
    "        d2 = self.dec2_conv(d2)\n",
    "        \n",
    "        # Output\n",
    "        output = torch.tanh(self.final_conv(d2))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bc49e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AudioDiscriminator(nn.Module):\n",
    "    \"\"\"Discriminatore per audio 1D\"\"\"\n",
    "    def __init__(self, input_channels=1, base_channels=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Conv1d(input_channels, base_channels, kernel_size=15, stride=2, padding=7),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Layer 2  \n",
    "            nn.Conv1d(base_channels, base_channels*2, kernel_size=15, stride=2, padding=7),\n",
    "            nn.BatchNorm1d(base_channels*2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Layer 3\n",
    "            nn.Conv1d(base_channels*2, base_channels*4, kernel_size=15, stride=2, padding=7),\n",
    "            nn.BatchNorm1d(base_channels*4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Layer 4\n",
    "            nn.Conv1d(base_channels*4, base_channels*8, kernel_size=15, stride=2, padding=7),\n",
    "            nn.BatchNorm1d(base_channels*8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Layer 5\n",
    "            nn.Conv1d(base_channels*8, base_channels*16, kernel_size=15, stride=2, padding=7),\n",
    "            nn.BatchNorm1d(base_channels*16),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        \n",
    "        # Global average pooling + classificazione\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(base_channels*16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.layers(x)\n",
    "        output = self.classifier(features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2397d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pydub_to_array(audio: AudioSegment) -> (np.ndarray, int):\n",
    "    return np.array(audio.get_array_of_samples(), dtype=np.float32).reshape((audio.channels, -1)) / (\n",
    "            1 << (8 * audio.sample_width - 1)), audio.frame_rate\n",
    "\n",
    "def array_to_pydub(audio_np_array: np.ndarray, sample_rate: int = 16000, sample_width: int = 2, channels: int = 1) -> AudioSegment:\n",
    "    return AudioSegment((audio_np_array * (2 ** (8 * sample_width - 1))).astype(np.int16).tobytes(),\n",
    "                        frame_rate=sample_rate, sample_width=sample_width, channels=channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19ea51c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_csv = \"train_dataset_wav.csv\"\n",
    "validation_csv = \"val_dataset_wav.csv\"\n",
    "test_csv = \"test_dataset_wav.csv\"\n",
    "\n",
    "# train_csv = 'pezz.csv'\n",
    "# validation_csv = 'pezz.csv'\n",
    "# test_csv = 'pezz.csv'\n",
    "\n",
    "train = AudioDataset(csv_file=train_csv)\n",
    "validation = AudioDataset(csv_file=validation_csv)\n",
    "test = AudioDataset(csv_file=test_csv)\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a473e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/10 | train G:0.2118 D:1.1485 || val G:0.2136 D:1.6362\n",
      "Saved best models (val G loss 0.2136)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02/10 | train G:0.1681 D:0.7624 || val G:0.1858 D:2.7815\n",
      "Saved best models (val G loss 0.1858)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03/10 | train G:0.1596 D:0.6032 || val G:0.1565 D:3.4864\n",
      "Saved best models (val G loss 0.1565)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04/10 | train G:0.1537 D:0.5417 || val G:0.1298 D:2.7105\n",
      "Saved best models (val G loss 0.1298)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05/10 | train G:0.1404 D:0.4661 || val G:0.1052 D:2.8606\n",
      "Saved best models (val G loss 0.1052)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06/10 | train G:0.1339 D:0.5403 || val G:0.0880 D:5.0187\n",
      "Saved best models (val G loss 0.0880)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07/10 | train G:0.1255 D:0.3899 || val G:0.0729 D:3.9754\n",
      "Saved best models (val G loss 0.0729)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08/10 | train G:0.1163 D:0.3236 || val G:0.0596 D:3.8117\n",
      "Saved best models (val G loss 0.0596)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09/10 | train G:0.1107 D:0.2770 || val G:0.0705 D:4.2961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | train G:0.1107 D:0.4047 || val G:0.0415 D:4.1080\n",
      "Saved best models (val G loss 0.0415)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------- modelli ----------\n",
    "generator = WaveUNetGenerator(input_channels=2, output_channels=1).to(device)\n",
    "discriminator = AudioDiscriminator(input_channels=1).to(device)\n",
    "\n",
    "# ---------- ottimizzatori ----------\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "\n",
    "# ---------- loss ----------\n",
    "bce_loss = nn.BCELoss()\n",
    "l1_loss  = nn.L1Loss()\n",
    "lambda_adv = 1e-3          # bilanciamento adversarial vs ricostruzione\n",
    "\n",
    "# ---------- training ----------\n",
    "n_epochs          = 10\n",
    "best_val_g_loss   = float(\"inf\")\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # ======== TRAIN ========\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    train_g_loss = 0.0\n",
    "    train_d_loss = 0.0\n",
    "\n",
    "    for mixture, background in tqdm(train_loader,\n",
    "                                    desc=f\"Epoch {epoch}/{n_epochs} [train]\",\n",
    "                                    leave=False):\n",
    "        mixture    = mixture.to(device)      # (B,1,T)\n",
    "        background = background.to(device)   # (B,1,T)\n",
    "        real_a     = (mixture - background).detach()\n",
    "\n",
    "        # ---- Generatore ----\n",
    "        pred_a = generator(mixture, background)\n",
    "        d_fake = discriminator(pred_a)\n",
    "        g_adv  = bce_loss(d_fake, torch.ones_like(d_fake))\n",
    "        g_rec  = l1_loss(pred_a, real_a)\n",
    "        g_loss = g_rec + lambda_adv * g_adv\n",
    "\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # ---- Discriminatore ----\n",
    "        with torch.no_grad():\n",
    "            fake_detached = pred_a.detach()\n",
    "        d_real = discriminator(real_a)\n",
    "        d_fake = discriminator(fake_detached)\n",
    "        d_loss = bce_loss(d_real, torch.ones_like(d_real)) + \\\n",
    "                 bce_loss(d_fake, torch.zeros_like(d_fake))\n",
    "\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        train_g_loss += g_loss.item()\n",
    "        train_d_loss += d_loss.item()\n",
    "\n",
    "    avg_train_g = train_g_loss / len(train_loader)\n",
    "    avg_train_d = train_d_loss / len(train_loader)\n",
    "\n",
    "    # ======== VALIDATION ========\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    val_g_loss = 0.0\n",
    "    val_d_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mixture, background in tqdm(validation_loader,\n",
    "                                        desc=f\"Epoch {epoch}/{n_epochs} [val ]\",\n",
    "                                        leave=False):\n",
    "            mixture    = mixture.to(device)\n",
    "            background = background.to(device)\n",
    "            real_a     = (mixture - background)\n",
    "\n",
    "            pred_a = generator(mixture, background)\n",
    "            d_fake = discriminator(pred_a)\n",
    "            g_adv  = bce_loss(d_fake, torch.ones_like(d_fake))\n",
    "            g_rec  = l1_loss(pred_a, real_a)\n",
    "            g_loss = g_rec + lambda_adv * g_adv\n",
    "            val_g_loss += g_loss.item()\n",
    "\n",
    "            # Discriminatore â€“ solo forward (no grad)\n",
    "            d_real = discriminator(real_a)\n",
    "            d_fake = discriminator(pred_a)\n",
    "            d_loss = bce_loss(d_real, torch.ones_like(d_real)) + \\\n",
    "                     bce_loss(d_fake, torch.zeros_like(d_fake))\n",
    "            val_d_loss += d_loss.item()\n",
    "\n",
    "    avg_val_g = val_g_loss / len(validation_loader)\n",
    "    avg_val_d = val_d_loss / len(validation_loader)\n",
    "\n",
    "    # ----- log -----\n",
    "    print(f\"Epoch {epoch:02d}/{n_epochs} | \"\n",
    "          f\"train G:{avg_train_g:.4f} D:{avg_train_d:.4f} || \"\n",
    "          f\"val G:{avg_val_g:.4f} D:{avg_val_d:.4f}\")\n",
    "\n",
    "    # ----- checkpoint sul miglior val_G -----\n",
    "    if avg_val_g < best_val_g_loss:\n",
    "        best_val_g_loss = avg_val_g\n",
    "        torch.save(generator.state_dict(), \"models/best_generator.pth\")\n",
    "        torch.save(discriminator.state_dict(), \"models/best_discriminator.pth\")\n",
    "        print(f\"Saved best models (val G loss {best_val_g_loss:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9e9af4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_loop(generator, discriminator, dataloader, device=\"cpu\", save_audio=False, save_path=\"outputs\"):\n",
    "    import os\n",
    "    from torchaudio import save as save_wav\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    generator.eval()\n",
    "    if discriminator:\n",
    "        discriminator.eval()\n",
    "\n",
    "    device = torch.device(device)\n",
    "    generator.to(device)\n",
    "    if discriminator:\n",
    "        discriminator.to(device)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, (mixture, _) in enumerate(dataloader):\n",
    "        mixture = mixture.to(device)  # (B, 1, T)\n",
    "\n",
    "        # Dummy source B per l'inferenza\n",
    "        source_b_dummy = torch.zeros_like(mixture[:, :1])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_source_a = generator(mixture, source_b_dummy)  # (B, 1, T)\n",
    "\n",
    "            if discriminator:\n",
    "                realism_score = discriminator(pred_source_a)\n",
    "            else:\n",
    "                realism_score = None\n",
    "\n",
    "        # Salva o colleziona risultati\n",
    "        for j in range(pred_source_a.size(0)):\n",
    "            idx = i * dataloader.batch_size + j\n",
    "            if save_audio:\n",
    "                filename = os.path.join(save_path, f\"predicted_v2{idx}.wav\")\n",
    "                save_wav(filename, pred_source_a[j].cpu(), sample_rate=16000)\n",
    "\n",
    "            results.append({\n",
    "                \"index\": idx,\n",
    "                \"realism_score\": realism_score[j].item() if realism_score is not None else None,\n",
    "                \"prediction\": pred_source_a[j].detach().cpu()\n",
    "            })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54870c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = WaveUNetGenerator(input_channels=2, output_channels=1)\n",
    "generator.load_state_dict(torch.load(\"models/best_generator.pth\"))\n",
    "discriminator = AudioDiscriminator(input_channels=1)\n",
    "discriminator.load_state_dict(torch.load(\"models/best_discriminator.pth\"))\n",
    "\n",
    "results = infer_loop(generator, discriminator, test_loader, device=\"cpu\", save_audio=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa4323f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
