{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T22:18:13.866523Z",
     "start_time": "2025-03-27T22:18:13.850772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchinfo\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pydub import AudioSegment\n",
    "import warnings\n",
    "import random\n",
    "import torch.nn.init as init\n",
    "from local_attention import LocalMHA\n",
    "from torchmetrics.audio import SignalDistortionRatio\n",
    "from demucs.hdemucs import HDemucs\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ],
   "id": "8ccbe583f5c3bccc",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T22:18:13.935813Z",
     "start_time": "2025-03-27T22:18:13.911722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class WaveNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.wave_enc1_conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.wave_enc1_norm1 = nn.BatchNorm1d(32)\n",
    "        self.wave_enc1_conv2 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=1)\n",
    "        self.wave_enc1_norm2 = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.wave_enc2_conv1 = nn.Conv1d(in_channels=32, out_channels=128, kernel_size=8, stride=4)\n",
    "        self.wave_enc2_norm1 = nn.BatchNorm1d(128)\n",
    "        self.wave_enc2_conv2 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=1)\n",
    "        self.wave_enc2_norm2 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.wave_enc3_conv1 = nn.Conv1d(in_channels=128, out_channels=512, kernel_size=8, stride=4)\n",
    "        self.wave_enc3_norm1 = nn.BatchNorm1d(512)\n",
    "        self.wave_enc3_conv2 = nn.Conv1d(in_channels=512, out_channels=512, kernel_size=1)\n",
    "        self.wave_enc3_norm2 = nn.BatchNorm1d(512)\n",
    "\n",
    "        self.wave_enc4_conv1 = nn.Conv1d(in_channels=512, out_channels=2048, kernel_size=8, stride=4)\n",
    "        self.wave_enc4_norm1 = nn.BatchNorm1d(2048)\n",
    "        self.wave_enc4_conv2 = nn.Conv1d(in_channels=2048, out_channels=2048, kernel_size=1)\n",
    "        self.wave_enc4_norm2 = nn.BatchNorm1d(2048)\n",
    "\n",
    "        # Bottleneck Attention\n",
    "        self.wave_bn_conv1 = nn.Conv1d(in_channels=2048, out_channels=1024, kernel_size=1)\n",
    "        self.wave_bn_norm1 = nn.BatchNorm1d(1024)\n",
    "        self.wave_bn_local_attention = LocalMHA(dim=1024, window_size=32, heads=4, dropout=0.3, causal=False,\n",
    "                                                  prenorm=False, exact_windowsize=False)\n",
    "        self.wave_bn_conv2 = nn.Conv1d(in_channels=1024, out_channels=1024, kernel_size=1)\n",
    "        self.wave_bn_norm2 = nn.BatchNorm1d(1024, affine=True)\n",
    "\n",
    "        # Decoder\n",
    "        self.wave_dec4_conv1 = nn.Conv1d(in_channels=1024, out_channels=512, kernel_size=1, stride=1)\n",
    "        self.wave_dec4_norm1 = nn.BatchNorm1d(512)\n",
    "        self.wave_dec4_conv2 = nn.Conv1d(2560, 512, kernel_size=1)\n",
    "        self.wave_dec4_norm2 = nn.BatchNorm1d(512)\n",
    "        self.wave_dec4_deconv = nn.ConvTranspose1d(in_channels=512, out_channels=512, kernel_size=8, stride=4)\n",
    "        self.wave_dec4_norm3 = nn.BatchNorm1d(512)\n",
    "\n",
    "        self.wave_dec3_conv1 = nn.Conv1d(in_channels=512, out_channels=128, kernel_size=1, stride=1)\n",
    "        self.wave_dec3_norm1 = nn.BatchNorm1d(128)\n",
    "        self.wave_dec3_conv2 = nn.Conv1d(640, 128, kernel_size=1)\n",
    "        self.wave_dec3_norm2 = nn.BatchNorm1d(128)\n",
    "        self.wave_dec3_deconv = nn.ConvTranspose1d(in_channels=128, out_channels=128, kernel_size=8, stride=4)\n",
    "        self.wave_dec3_norm3 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.wave_dec2_conv1 = nn.Conv1d(in_channels=128, out_channels=32, kernel_size=1, stride=1)\n",
    "        self.wave_dec2_norm1 = nn.BatchNorm1d(32)\n",
    "        self.wave_dec2_conv2 = nn.Conv1d(160, 32, kernel_size=1)\n",
    "        self.wave_dec2_norm2 = nn.BatchNorm1d(32)\n",
    "        self.wave_dec2_deconv = nn.ConvTranspose1d(in_channels=32, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.wave_dec2_norm3 = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.wave_dec1_conv1 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=1, stride=1)\n",
    "        self.wave_dec1_norm1 = nn.BatchNorm1d(16)\n",
    "        self.wave_dec1_deconv = nn.ConvTranspose1d(in_channels=48, out_channels=16, kernel_size=8, stride=4)\n",
    "        self.wave_dec1_norm2 = nn.BatchNorm1d(16)\n",
    "        self.wave_dec1_conv2 = nn.Conv1d(16, 16, kernel_size=1)\n",
    "        self.wave_dec1_norm3 = nn.BatchNorm1d(16)\n",
    "\n",
    "        # Foreground/Background Separation\n",
    "        self.wave_conv_foreground = nn.Conv1d(in_channels=16, out_channels=1, kernel_size=1)\n",
    "        self.wave_norm_foreground = nn.BatchNorm1d(1)\n",
    "\n",
    "        self.wave_conv_background = nn.Conv1d(in_channels=16, out_channels=1, kernel_size=1)\n",
    "        self.wave_norm_background = nn.BatchNorm1d(1)\n",
    "\n",
    "        # Output\n",
    "        self.wave_output_conv_foreground = nn.Conv1d(in_channels=2, out_channels=1, kernel_size=1)\n",
    "        self.wave_output_norm_foreground = nn.BatchNorm1d(1)\n",
    "\n",
    "        self.wave_output_conv_background = nn.Conv1d(in_channels=2, out_channels=1, kernel_size=1)\n",
    "        self.wave_output_norm_background = nn.BatchNorm1d(1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        wave_res1 = self.wave_enc1_conv1(x)\n",
    "        wave_res1 = self.wave_enc1_norm1(wave_res1)\n",
    "        wave_res1 = F.gelu(wave_res1)\n",
    "        wave_res1 = self.wave_enc1_conv2(wave_res1)\n",
    "        wave_res1 = self.wave_enc1_norm2(wave_res1)\n",
    "        wave_e1 = F.gelu(wave_res1) + wave_res1\n",
    "\n",
    "        wave_res2 = self.wave_enc2_conv1(wave_e1)\n",
    "        wave_res2 = self.wave_enc2_norm1(wave_res2)\n",
    "        wave_res2 = F.gelu(wave_res2)\n",
    "        wave_e2 = self.wave_enc2_conv2(wave_res2)\n",
    "        wave_e2 = self.wave_enc2_norm2(wave_e2)\n",
    "        wave_e2 = F.gelu(wave_e2) + wave_res2\n",
    "\n",
    "        wave_res3 = self.wave_enc3_conv1(wave_e2)\n",
    "        wave_res3 = self.wave_enc3_norm1(wave_res3)\n",
    "        wave_res3 = F.gelu(wave_res3)\n",
    "        wave_e3 = self.wave_enc3_conv2(wave_res3)\n",
    "        wave_e3 = self.wave_enc3_norm2(wave_e3)\n",
    "        wave_e3 = F.gelu(wave_e3) + wave_res3\n",
    "\n",
    "        wave_res4 = self.wave_enc4_conv1(wave_e3)\n",
    "        wave_res4 = self.wave_enc4_norm1(wave_res4)\n",
    "        wave_res4 = F.gelu(wave_res4)\n",
    "        wave_e4 = self.wave_enc4_conv2(wave_res4)\n",
    "        wave_e4 = self.wave_enc4_norm2(wave_e4)\n",
    "        wave_e4 = F.gelu(wave_e4) + wave_res4\n",
    "\n",
    "        #Bottleneck\n",
    "        wave_bn_res = self.wave_bn_conv1(wave_e4)\n",
    "        wave_bn_res = self.wave_bn_norm1(wave_bn_res)\n",
    "        wave_bn_res = F.gelu(wave_bn_res)\n",
    "        wave_bn =  wave_bn_res.permute(0, 2, 1)\n",
    "        wave_bn = self.wave_bn_local_attention(wave_bn)\n",
    "        wave_bn = wave_bn.permute(0, 2, 1)\n",
    "        wave_bn = self.wave_bn_conv2(wave_bn)\n",
    "        wave_bn = self.wave_bn_norm2(wave_bn)\n",
    "        wave_bn = F.gelu(wave_bn) + wave_bn_res\n",
    "\n",
    "        ### Decoder ###\n",
    "        # Decoder Layer 4\n",
    "        wave_res4 = self.wave_dec4_conv1(wave_bn)\n",
    "        wave_res4 = self.wave_dec4_norm1(wave_res4)\n",
    "        wave_res4 = F.gelu(wave_res4)\n",
    "        wave_res4 = F.pad(wave_res4, (0, wave_e4.shape[-1] - wave_res4.shape[-1]))  # Padding per matching dimensionale\n",
    "        wave_d4 = torch.cat([wave_res4, wave_e4], dim=1)  # Concatenazione con skip connection\n",
    "        wave_d4 = self.wave_dec4_conv2(wave_d4)\n",
    "        wave_d4 = self.wave_dec4_norm2(wave_d4)\n",
    "        wave_d4 = F.gelu(wave_d4) + wave_res4  # Residual connection\n",
    "        wave_d4 = self.wave_dec4_deconv(wave_d4)  # Upsampling\n",
    "        wave_d4 = self.wave_dec4_norm3(wave_d4)\n",
    "        wave_d4 = F.gelu(wave_d4)\n",
    "\n",
    "        # Decoder Layer 3\n",
    "        wave_res3 = self.wave_dec3_conv1(wave_d4)\n",
    "        wave_res3 = self.wave_dec3_norm1(wave_res3)\n",
    "        wave_res3 = F.gelu(wave_res3)\n",
    "        wave_res3 = F.pad(wave_res3, (0, wave_e3.shape[-1] - wave_res3.shape[-1]))  # Padding per matching dimensionale\n",
    "        wave_d3 = torch.cat([wave_res3, wave_e3], dim=1)  # Concatenazione con skip connection\n",
    "        wave_d3 = self.wave_dec3_conv2(wave_d3)\n",
    "        wave_d3 = self.wave_dec3_norm2(wave_d3)\n",
    "        wave_d3 = F.gelu(wave_d3) + wave_res3  # Residual connection\n",
    "        wave_d3 = self.wave_dec3_deconv(wave_d3)  # Upsampling\n",
    "        wave_d3 = self.wave_dec3_norm3(wave_d3)\n",
    "        wave_d3 = F.gelu(wave_d3)\n",
    "\n",
    "        # Decoder Layer 2\n",
    "        wave_res2 = self.wave_dec2_conv1(wave_d3)\n",
    "        wave_res2 = self.wave_dec2_norm1(wave_res2)\n",
    "        wave_res2 = F.gelu(wave_res2)\n",
    "        wave_res2 = F.pad(wave_res2, (0, wave_e2.shape[-1] - wave_res2.shape[-1]))  # Padding per matching dimensionale\n",
    "        wave_d2 = torch.cat([wave_res2, wave_e2], dim=1)  # Concatenazione con skip connection\n",
    "        wave_d2 = self.wave_dec2_conv2(wave_d2)\n",
    "        wave_d2 = self.wave_dec2_norm2(wave_d2)\n",
    "        wave_d2 = F.gelu(wave_d2) + wave_res2  # Residual connection\n",
    "        wave_d2 = self.wave_dec2_deconv(wave_d2)  # Upsampling\n",
    "        wave_d2 = self.wave_dec2_norm3(wave_d2)\n",
    "        wave_d2 = F.gelu(wave_d2)\n",
    "\n",
    "        # Decoder Layer 1\n",
    "        wave_d2 = self.wave_dec1_conv1(wave_d2)\n",
    "        wave_d2 = self.wave_dec1_norm1(wave_d2)\n",
    "        wave_d2 = F.gelu(wave_d2)\n",
    "        wave_d2 = torch.cat([F.pad(wave_d2, (0, wave_e1.shape[-1] - wave_d2.shape[-1])), wave_e1], dim=1)  # Concatenazione con skip connection\n",
    "        wave_res1 = self.wave_dec1_deconv(wave_d2)  # Upsampling\n",
    "        wave_res1 = self.wave_dec1_norm2(wave_res1)\n",
    "        wave_res1 = F.gelu(wave_res1)\n",
    "\n",
    "        wave_d1 = self.wave_dec1_conv2(wave_res1)\n",
    "        wave_d1 = self.wave_dec1_norm3(wave_d1)\n",
    "        wave_d1 = F.gelu(wave_d1) + wave_res1  # Residual connection\n",
    "\n",
    "\n",
    "        # Foreground/Background Separation\n",
    "        wave_foreground = self.wave_conv_foreground(wave_d1)\n",
    "        wave_background = self.wave_conv_background(wave_d1)\n",
    "        wave_foreground = self.wave_norm_foreground(wave_foreground)\n",
    "        wave_background = self.wave_norm_background(wave_background)\n",
    "        wave_foreground = F.gelu(wave_foreground)\n",
    "        wave_background = F.gelu(wave_background)\n",
    "\n",
    "        # Output\n",
    "        foreground = self.wave_output_conv_foreground(torch.cat([wave_foreground, x], dim=1))\n",
    "        background = self.wave_output_conv_background(torch.cat([wave_background, x], dim=1))\n",
    "        foreground =  F.gelu(foreground)\n",
    "        background = F.gelu(background)\n",
    "        foreground = self.wave_output_norm_foreground(foreground)\n",
    "        background = self.wave_output_norm_background(background)\n",
    "        foreground = F.tanh(foreground)\n",
    "        background = F.tanh(background)\n",
    "\n",
    "        return foreground, background"
   ],
   "id": "a2e82f1b91c3ae66",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-27T22:18:14.002429Z",
     "start_time": "2025-03-27T22:18:13.979341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class VoxStrideNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder (rimane invariato)\n",
    "        self.e1_conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.e1_in1 = nn.InstanceNorm1d(32, affine=True)\n",
    "        self.e1_conv2 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=1)\n",
    "        self.e1_in2 = nn.InstanceNorm1d(32, affine=True)\n",
    "\n",
    "        self.e2_conv1 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=8, stride=4)\n",
    "        self.e2_in1 = nn.InstanceNorm1d(64, affine=True)\n",
    "        self.e2_bilstm = nn.LSTM(input_size=64, hidden_size=64, bidirectional=True, batch_first=True)\n",
    "        self.e2_conv2 = nn.Conv1d(in_channels=128, out_channels=64, kernel_size=1)\n",
    "        self.e2_in2 = nn.InstanceNorm1d(64, affine=True)\n",
    "\n",
    "        self.e3_conv1 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=8, stride=4)\n",
    "        self.e3_in1 = nn.InstanceNorm1d(128, affine=True)\n",
    "        self.e3_bilstm = nn.LSTM(input_size=128, hidden_size=128, bidirectional=True, batch_first=True)\n",
    "        self.e3_conv2 = nn.Conv1d(in_channels=256, out_channels=128, kernel_size=1)\n",
    "        self.e3_in2 = nn.InstanceNorm1d(128, affine=True)\n",
    "\n",
    "        self.e4_conv1 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=8, stride=4)\n",
    "        self.e4_in1 = nn.InstanceNorm1d(256, affine=True)\n",
    "        self.e4_bilstm = nn.LSTM(input_size=256, hidden_size=256, bidirectional=True, batch_first=True)\n",
    "        self.e4_conv2 = nn.Conv1d(in_channels=512, out_channels=256, kernel_size=1)\n",
    "        self.e4_in2 = nn.InstanceNorm1d(256, affine=True)\n",
    "\n",
    "        self.e5_conv1 = nn.Conv1d(in_channels=256, out_channels=512, kernel_size=8, stride=4)\n",
    "        self.e5_in1 = nn.InstanceNorm1d(512, affine=True)\n",
    "        self.e5_bilstm = nn.LSTM(input_size=512, hidden_size=512, bidirectional=True, batch_first=True)\n",
    "        self.e5_conv2 = nn.Conv1d(in_channels=1024, out_channels=512, kernel_size=1)\n",
    "        self.e5_in2 = nn.InstanceNorm1d(512, affine=True)\n",
    "\n",
    "        # Decoder\n",
    "        self.d5_conv1 = nn.Conv1d(in_channels=512, out_channels=256, kernel_size=1, stride=1)\n",
    "        self.d5_in1 = nn.InstanceNorm1d(256, affine=True)\n",
    "        self.d5_deconv1 = nn.ConvTranspose1d(in_channels=512 + 256, out_channels=256, kernel_size=8, stride=4)\n",
    "        self.d5_in2 = nn.InstanceNorm1d(256, affine=True)\n",
    "        self.d5_lstm = nn.LSTM(input_size=256, hidden_size=256, batch_first=True)\n",
    "        self.d5_conv2 = nn.Conv1d(256, 256, kernel_size=1)\n",
    "        self.d5_in3 = nn.InstanceNorm1d(256, affine=True)\n",
    "\n",
    "        self.d4_conv1 = nn.Conv1d(in_channels=256, out_channels=128, kernel_size=1, stride=1)\n",
    "        self.d4_in1 = nn.InstanceNorm1d(128, affine=True)\n",
    "        self.d4_deconv1 = nn.ConvTranspose1d(in_channels=256 + 128, out_channels=128, kernel_size=8, stride=4)\n",
    "        self.d4_in2 = nn.InstanceNorm1d(128, affine=True)\n",
    "        self.d4_lstm = nn.LSTM(input_size=128, hidden_size=128, batch_first=True, bidirectional=True)\n",
    "        self.d4_conv2 = nn.Conv1d(256, 128, kernel_size=1)\n",
    "        self.d4_in3 = nn.InstanceNorm1d(128, affine=True)\n",
    "\n",
    "        self.d3_conv1 = nn.Conv1d(in_channels=128, out_channels=64, kernel_size=1, stride=1)\n",
    "        self.d3_in1 = nn.InstanceNorm1d(64, affine=True)\n",
    "        self.d3_deconv1 = nn.ConvTranspose1d(in_channels=128 + 64, out_channels=64, kernel_size=8, stride=4)\n",
    "        self.d3_in2 = nn.InstanceNorm1d(64, affine=True)\n",
    "        self.d3_lstm = nn.LSTM(input_size=64, hidden_size=64, batch_first=True)\n",
    "        self.d3_conv2 = nn.Conv1d(64, 64, kernel_size=1)\n",
    "        self.d3_in3 = nn.InstanceNorm1d(64, affine=True)\n",
    "\n",
    "        self.d2_conv1 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=1, stride=1)\n",
    "        self.d2_in1 = nn.InstanceNorm1d(32, affine=True)\n",
    "        self.d2_deconv1 = nn.ConvTranspose1d(in_channels=64 + 32, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.d2_in2 = nn.InstanceNorm1d(32, affine=True)\n",
    "        self.d2_lstm = nn.LSTM(input_size=32, hidden_size=32, batch_first=True)\n",
    "        self.d2_conv2 = nn.Conv1d(32, 32, kernel_size=1)\n",
    "        self.d2_in3 = nn.InstanceNorm1d(32, affine=True)\n",
    "\n",
    "        self.d1_conv1 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=1, stride=1)\n",
    "        self.d1_in1 = nn.InstanceNorm1d(16, affine=True)\n",
    "        self.d1_deconv1 = nn.ConvTranspose1d(in_channels=32 + 16, out_channels=16, kernel_size=8, stride=4)\n",
    "        self.d1_in2 = nn.InstanceNorm1d(16, affine=True)\n",
    "        self.d1_conv2 = nn.Conv1d(16, 16, kernel_size=1)\n",
    "        self.d1_in3 = nn.InstanceNorm1d(16, affine=True)\n",
    "\n",
    "        # Output\n",
    "        self.output_conv_event = nn.Conv1d(in_channels=16, out_channels=1, kernel_size=1)\n",
    "        self.output_in_event = nn.InstanceNorm1d(1, affine=True)\n",
    "        self.output_conv_background = nn.Conv1d(in_channels=16, out_channels=1, kernel_size=1)\n",
    "        self.output_in_background = nn.InstanceNorm1d(1, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = F.leaky_relu(self.e1_in1(self.e1_conv1(x)))\n",
    "        res1 = e1\n",
    "        e1 = F.gelu(self.e1_in2(self.e1_conv2(res1))) + res1\n",
    "\n",
    "        e2 = F.leaky_relu(self.e2_in1(self.e2_conv1(e1)))\n",
    "        res2 = e2\n",
    "        e2 = e2.permute(0, 2, 1)\n",
    "        e2, _ = self.e2_bilstm(e2)\n",
    "        e2 = e2.permute(0, 2, 1)\n",
    "        e2 = F.gelu(self.e2_in2(self.e2_conv2(e2))) + res2\n",
    "\n",
    "        e3 = F.leaky_relu(self.e3_in1(self.e3_conv1(e2)))\n",
    "        res3 = e3\n",
    "        e3 = e3.permute(0, 2, 1)\n",
    "        e3, _ = self.e3_bilstm(e3)\n",
    "        e3 = e3.permute(0, 2, 1)\n",
    "        e3 = F.gelu(self.e3_in2(self.e3_conv2(e3))) + res3\n",
    "\n",
    "        e4 = F.leaky_relu(self.e4_in1(self.e4_conv1(e3)))\n",
    "        res4 = e4\n",
    "        e4 = e4.permute(0, 2, 1)\n",
    "        e4, _ = self.e4_bilstm(e4)\n",
    "        e4 = e4.permute(0, 2, 1)\n",
    "        e4 = F.gelu(self.e4_in2(self.e4_conv2(e4))) + res4\n",
    "\n",
    "        e5 = F.leaky_relu(self.e5_in1(self.e5_conv1(e4)))\n",
    "        res5 = e5\n",
    "        e5 = e5.permute(0, 2, 1)\n",
    "        e5, _ = self.e5_bilstm(e5)\n",
    "        e5 = e5.permute(0, 2, 1)\n",
    "        e5 = F.gelu(self.e5_in2(self.e5_conv2(e5))) + res5\n",
    "\n",
    "        d5 = F.leaky_relu(self.d5_in1(self.d5_conv1(e5)))\n",
    "        d5 = torch.cat([F.pad(d5, (0, e5.shape[-1] - d5.shape[-1])), e5], dim=1)\n",
    "        res5 = F.leaky_relu(self.d5_in2(self.d5_deconv1(d5)))\n",
    "        d5 = res5.permute(0, 2, 1)\n",
    "        d5, _ = self.d5_lstm(d5)\n",
    "        d5 = d5.permute(0, 2, 1)\n",
    "        d5 = F.leaky_relu(self.d5_in3(self.d5_conv2(d5))) + res5\n",
    "\n",
    "        d4 = F.leaky_relu(self.d4_in1(self.d4_conv1(d5)))\n",
    "        d4 = torch.cat([F.pad(d4, (0, e4.shape[-1] - d4.shape[-1])), e4], dim=1)\n",
    "        res4 = F.leaky_relu(self.d4_in2(self.d4_deconv1(d4)))\n",
    "        d4 = res4.permute(0, 2, 1)\n",
    "        d4, _ = self.d4_lstm(d4)\n",
    "        d4 = d4.permute(0, 2, 1)\n",
    "        d4 = F.leaky_relu(self.d4_in3(self.d4_conv2(d4))) + res4\n",
    "\n",
    "        d3 = F.leaky_relu(self.d3_in1(self.d3_conv1(d4)))\n",
    "        d3 = torch.cat([F.pad(d3, (0, e3.shape[-1] - d3.shape[-1])), e3], dim=1)\n",
    "        res3 = F.leaky_relu(self.d3_in2(self.d3_deconv1(d3)))\n",
    "        d3 = res3.permute(0, 2, 1)\n",
    "        d3, _ = self.d3_lstm(d3)\n",
    "        d3 = d3.permute(0, 2, 1)\n",
    "        d3 = F.leaky_relu(self.d3_in3(self.d3_conv2(d3))) + res3\n",
    "\n",
    "        d2 = F.leaky_relu(self.d2_in1(self.d2_conv1(d3)))\n",
    "        d2 = torch.cat([F.pad(d2, (0, e2.shape[-1] - d2.shape[-1])), e2], dim=1)\n",
    "        res2 = F.leaky_relu(self.d2_in2(self.d2_deconv1(d2)))\n",
    "        d2 = res2.permute(0, 2, 1)\n",
    "        d2, _ = self.d2_lstm(d2)\n",
    "        d2 = d2.permute(0, 2, 1)\n",
    "        d2 = F.leaky_relu(self.d2_in3(self.d2_conv2(d2))) + res2\n",
    "\n",
    "        d1 = F.leaky_relu(self.d1_in1(self.d1_conv1(d2)))\n",
    "        d1 = torch.cat([F.pad(d1, (0, e1.shape[-1] - d1.shape[-1])), e1], dim=1)\n",
    "        res1 = F.leaky_relu(self.d1_in2(self.d1_deconv1(d1)))\n",
    "        d1 = F.leaky_relu(self.d1_in3(self.d1_conv2(res1))) + res1\n",
    "\n",
    "        # Output\n",
    "        event = F.tanh(self.output_in_event(self.output_conv_event(d1)))\n",
    "        background = F.tanh(self.output_in_background(self.output_conv_background(d1)))\n",
    "\n",
    "        return event, background"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T22:18:14.053533Z",
     "start_time": "2025-03-27T22:18:14.047002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, target_duration=10000, target_sample_rate=10000,\n",
    "                 target_channels=1):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.target_duration = target_duration\n",
    "        self.target_channels = target_channels\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        event_path = os.path.join(self.df.loc[idx, 'event'])\n",
    "        background_path = os.path.join(self.df.loc[idx, 'background'])\n",
    "        mixture_path = os.path.join(self.df.loc[idx, 'mixture'])\n",
    "\n",
    "        event_audio = AudioSegment.from_file(event_path).set_channels(self.target_channels).set_frame_rate(self.target_sample_rate)\n",
    "        background_audio = AudioSegment.from_file(background_path).set_channels(self.target_channels).set_frame_rate(self.target_sample_rate)\n",
    "        mixture_audio = AudioSegment.from_file(mixture_path).set_channels(self.target_channels).set_frame_rate(self.target_sample_rate)\n",
    "\n",
    "        event, _ = self._pydub_to_array(event_audio)\n",
    "        background, _ = self._pydub_to_array(background_audio)\n",
    "        mixture, _ = self._pydub_to_array(mixture_audio)\n",
    "\n",
    "        event_tensor = torch.Tensor(event)\n",
    "        background_tensor = torch.Tensor(background)\n",
    "        mixture_tensor = torch.Tensor(mixture)\n",
    "\n",
    "        return mixture_tensor, event_tensor, background_tensor\n",
    "\n",
    "    def _pydub_to_array(self, audio: AudioSegment) -> (np.ndarray, int):\n",
    "        return np.array(audio.get_array_of_samples(), dtype=np.float32).reshape((audio.channels, -1)) / (\n",
    "                1 << (8 * audio.sample_width - 1)), audio.frame_rate"
   ],
   "id": "69355f136c5d46a6",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T22:18:14.102505Z",
     "start_time": "2025-03-27T22:18:14.098740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pydub_to_array(audio: AudioSegment) -> (np.ndarray, int):\n",
    "    return np.array(audio.get_array_of_samples(), dtype=np.float32).reshape((audio.channels, -1)) / (\n",
    "            1 << (8 * audio.sample_width - 1)), audio.frame_rate\n",
    "\n",
    "def array_to_pydub(audio_np_array: np.ndarray, sample_rate: int = 10000, sample_width: int = 2, channels: int = 1) -> AudioSegment:\n",
    "    return AudioSegment((audio_np_array * (2 ** (8 * sample_width - 1))).astype(np.int16).tobytes(),\n",
    "                        frame_rate=sample_rate, sample_width=sample_width, channels=channels)"
   ],
   "id": "a3e3ed16cab5044e",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T22:18:14.343102Z",
     "start_time": "2025-03-27T22:18:14.147165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_csv = \"utils/train_dataset_wav.csv\"\n",
    "validation_csv = \"utils/val_dataset_wav.csv\"\n",
    "test_csv = \"utils/test_dataset_wav.csv\"\n",
    "\n",
    "# train_csv = 'pezz.csv'\n",
    "# validation_csv = 'pezz.csv'\n",
    "# test_csv = 'pezz.csv'\n",
    "\n",
    "train = AudioDataset(csv_file=train_csv)\n",
    "validation = AudioDataset(csv_file=validation_csv)\n",
    "test = AudioDataset(csv_file=test_csv)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)"
   ],
   "id": "af0158714201b47f",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T22:18:14.548180Z",
     "start_time": "2025-03-27T22:18:14.373898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = VoxStrideNet().to(device)\n",
    "model = HDemucs(audio_channels=1, channels=24, sources=['event', 'background']).to(device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "sdr_metric = SignalDistortionRatio().to(device)"
   ],
   "id": "700a3d0e0defa771",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T22:18:34.664330Z",
     "start_time": "2025-03-27T22:18:14.573587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#File CSV per il log delle metriche\n",
    "csv_filename = \"HD_sep_training_log.csv\"\n",
    "\n",
    "# Crea un DataFrame vuoto per il log\n",
    "if os.path.exists(csv_filename):\n",
    "    log_df = pd.read_csv(csv_filename)\n",
    "else:\n",
    "    log_df = pd.DataFrame(columns=[\"epoch\", \"train_loss\", \"train_sdr\", \"val_loss\", \"val_sdr\"])\n",
    "\n",
    "\n",
    "# Numero di epoche\n",
    "num_epochs = 20\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "# Loop di training\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Imposta il modello in modalità training\n",
    "    train_running_loss = 0.0\n",
    "    train_running_sdr = 0.0\n",
    "\n",
    "    # Usa tqdm per una barra di avanzamento\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Train\", unit=\"batch\") as train_bar:\n",
    "        for mixture, event, background in train_bar:\n",
    "            train_bar.set_description(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "            # Sposta i dati sul dispositivo corretto\n",
    "            mixture = mixture.to(device)\n",
    "            event = event.to(device)\n",
    "            background = background.to(device)\n",
    "\n",
    "            # Azzera i gradienti\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(mixture)\n",
    "\n",
    "            if isinstance(output, tuple):\n",
    "                event_pred, background_pred = output\n",
    "            else:\n",
    "                event_pred, background_pred = output[:, 0, :, :], output[:, 1, :, :]\n",
    "\n",
    "\n",
    "            # Calcola la loss\n",
    "            loss_event = criterion(event_pred, event)\n",
    "            loss_background = criterion(background_pred, background)\n",
    "            loss = loss_event + loss_background\n",
    "\n",
    "            # Backward pass e ottimizzazione\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calcola l'SDR\n",
    "            sdr_event = sdr_metric(event_pred, event)\n",
    "            sdr_background = sdr_metric(background_pred, background)\n",
    "            sdr = (sdr_event + sdr_background) / 2  # Media dell'SDR per evento e background\n",
    "\n",
    "            # Aggiorna la loss e l'SDR totali\n",
    "            train_running_loss += loss.item()\n",
    "            train_running_sdr += sdr.item()\n",
    "\n",
    "            # Aggiorna la barra di avanzamento con loss e SDR\n",
    "            train_bar.set_postfix(loss=train_running_loss / (train_bar.n + 1), sdr=train_running_sdr / (train_bar.n + 1))\n",
    "\n",
    "    train_running_loss /= len(train_loader)\n",
    "    train_running_sdr /= len(train_loader)\n",
    "\n",
    "    # Validation loop (opzionale)\n",
    "    model.eval()  # Imposta il modello in modalità evaluation\n",
    "    val_running_loss = 0.0\n",
    "    val_running_sdr = 0.0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(validation_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Validation\", unit=\"batch\") as val_bar:\n",
    "            for mixture, event, background in val_bar:\n",
    "                    mixture = mixture.to(device)\n",
    "                    event = event.to(device)\n",
    "                    background = background.to(device)\n",
    "\n",
    "                    output = model(mixture)\n",
    "\n",
    "                    if isinstance(output, tuple):\n",
    "                        event_pred, background_pred = output\n",
    "                    else:\n",
    "                        event_pred, background_pred = output[:, 0, :, :], output[:, 1, :, :]\n",
    "\n",
    "\n",
    "                    loss_event = criterion(event_pred, event)\n",
    "                    loss_background = criterion(background_pred, background)\n",
    "                    loss = loss_event + loss_background\n",
    "\n",
    "                    sdr_event = sdr_metric(event_pred, event)\n",
    "                    sdr_background = sdr_metric(background_pred, background)\n",
    "                    sdr = (sdr_event + sdr_background) / 2\n",
    "\n",
    "                    val_running_loss += loss.item()\n",
    "                    val_running_sdr += sdr.item()\n",
    "\n",
    "                    val_bar.set_postfix(loss=val_running_loss / (val_bar.n + 1), val_sdr=val_running_sdr / (val_bar.n + 1))\n",
    "\n",
    "        val_running_loss /= len(validation_loader)\n",
    "        val_running_sdr /= len(validation_loader)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_running_loss:.4f}, Train SDR: {train_running_sdr:.4f}')\n",
    "    print(\"-\" * 70)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {val_running_loss:.4f}, Validation SDR: {val_running_sdr:.4f}')\n",
    "\n",
    "\n",
    "     # Aggiorna il log\n",
    "    new_row = pd.DataFrame({\n",
    "        \"epoch\": [epoch + 1],\n",
    "        \"train_loss\": [train_running_loss],\n",
    "        \"train_sdr\": [train_running_sdr],\n",
    "        \"val_loss\": [val_running_loss],\n",
    "        \"val_sdr\": [val_running_sdr]})\n",
    "\n",
    "    log_df = pd.concat([log_df, new_row], ignore_index=True)\n",
    "    log_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "    # Salvataggio modello se migliora\n",
    "    if val_running_loss < best_val_loss:\n",
    "        best_val_loss = val_running_loss\n",
    "        torch.save(model.state_dict(), \"HD_sep_best_model.pth\")\n",
    "        print(f\"Best Model Saved (Epoch {epoch+1}, Val Loss: {val_running_loss:.4f})\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "print(f\"Training log salvato in {csv_filename}\")"
   ],
   "id": "4d69a7ce34f8efa3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   0%|          | 21/8750 [00:19<2:17:01,  1.06batch/s, loss=0.189, sdr=-11.6]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 23\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Usa tqdm per una barra di avanzamento\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tqdm(train_loader, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m - Train\u001B[39m\u001B[38;5;124m\"\u001B[39m, unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m train_bar:\n\u001B[0;32m---> 23\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m mixture, event, background \u001B[38;5;129;01min\u001B[39;00m train_bar:\n\u001B[1;32m     24\u001B[0m         train_bar\u001B[38;5;241m.\u001B[39mset_description(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     26\u001B[0m         \u001B[38;5;66;03m# Sposta i dati sul dispositivo corretto\u001B[39;00m\n",
      "File \u001B[0;32m/media/neurone-pc4/Data/Milone/SED/.venv/lib/python3.8/site-packages/tqdm/std.py:1181\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1178\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[1;32m   1180\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1181\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[1;32m   1182\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[1;32m   1183\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[1;32m   1184\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[0;32m/media/neurone-pc4/Data/Milone/SED/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m/media/neurone-pc4/Data/Milone/SED/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:673\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    671\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    672\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 673\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    674\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    675\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m/media/neurone-pc4/Data/Milone/SED/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m/media/neurone-pc4/Data/Milone/SED/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[0;32mIn[18], line 18\u001B[0m, in \u001B[0;36mAudioDataset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     15\u001B[0m background_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf\u001B[38;5;241m.\u001B[39mloc[idx, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbackground\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     16\u001B[0m mixture_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf\u001B[38;5;241m.\u001B[39mloc[idx, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmixture\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m---> 18\u001B[0m event_audio \u001B[38;5;241m=\u001B[39m \u001B[43mAudioSegment\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevent_path\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mset_channels(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_channels)\u001B[38;5;241m.\u001B[39mset_frame_rate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_sample_rate)\n\u001B[1;32m     19\u001B[0m background_audio \u001B[38;5;241m=\u001B[39m AudioSegment\u001B[38;5;241m.\u001B[39mfrom_file(background_path)\u001B[38;5;241m.\u001B[39mset_channels(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_channels)\u001B[38;5;241m.\u001B[39mset_frame_rate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_sample_rate)\n\u001B[1;32m     20\u001B[0m mixture_audio \u001B[38;5;241m=\u001B[39m AudioSegment\u001B[38;5;241m.\u001B[39mfrom_file(mixture_path)\u001B[38;5;241m.\u001B[39mset_channels(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_channels)\u001B[38;5;241m.\u001B[39mset_frame_rate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_sample_rate)\n",
      "File \u001B[0;32m/media/neurone-pc4/Data/Milone/SED/.venv/lib/python3.8/site-packages/pydub/audio_segment.py:651\u001B[0m, in \u001B[0;36mAudioSegment.from_file\u001B[0;34m(cls, file, format, codec, parameters, start_second, duration, **kwargs)\u001B[0m\n\u001B[1;32m    649\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m    650\u001B[0m     filename \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 651\u001B[0m file, close_file \u001B[38;5;241m=\u001B[39m \u001B[43m_fd_or_path_or_tempfile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtempfile\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    653\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m:\n\u001B[1;32m    654\u001B[0m     \u001B[38;5;28mformat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m.\u001B[39mlower()\n",
      "File \u001B[0;32m/media/neurone-pc4/Data/Milone/SED/.venv/lib/python3.8/site-packages/pydub/utils.py:60\u001B[0m, in \u001B[0;36m_fd_or_path_or_tempfile\u001B[0;34m(fd, mode, tempfile)\u001B[0m\n\u001B[1;32m     57\u001B[0m     close_fd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(fd, basestring):\n\u001B[0;32m---> 60\u001B[0m     fd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     61\u001B[0m     close_fd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "14c884ac351d64a7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
