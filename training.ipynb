{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e95875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pydub import AudioSegment\n",
    "import pandas as pd\n",
    "from torchmetrics.audio import SignalDistortionRatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa91a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, target_duration=10000, target_sample_rate=16000,\n",
    "                 target_channels=1):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.target_duration = target_duration\n",
    "        self.target_channels = target_channels\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        background_path = os.path.join(self.df.loc[idx, 'background'])\n",
    "        mixture_path = os.path.join(self.df.loc[idx, 'mixture'])\n",
    "\n",
    "        background_audio = AudioSegment.from_file(background_path).set_channels(self.target_channels).set_frame_rate(self.target_sample_rate)\n",
    "        mixture_audio = AudioSegment.from_file(mixture_path).set_channels(self.target_channels).set_frame_rate(self.target_sample_rate)\n",
    "\n",
    "        background, _ = self._pydub_to_array(background_audio)\n",
    "        mixture, _ = self._pydub_to_array(mixture_audio)\n",
    "\n",
    "        background_tensor = torch.Tensor(background)\n",
    "        mixture_tensor = torch.Tensor(mixture)\n",
    "\n",
    "        return mixture_tensor, background_tensor\n",
    "\n",
    "    def _pydub_to_array(self, audio: AudioSegment) -> (np.ndarray, int):\n",
    "        return np.array(audio.get_array_of_samples(), dtype=np.float32).reshape((audio.channels, -1)) / (\n",
    "                1 << (8 * audio.sample_width - 1)), audio.frame_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd618df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveUNetBlock(nn.Module):\n",
    "    \"\"\"Blocco base per Wave-U-Net con convoluzione 1D\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=15, stride=1, dilation=1):\n",
    "        super().__init__()\n",
    "        padding = (kernel_size - 1) // 2 * dilation\n",
    "        \n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, \n",
    "                             stride=stride, padding=padding, dilation=dilation)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.activation(self.bn(self.conv(x)))\n",
    "\n",
    "class WaveUNetGenerator(nn.Module):\n",
    "    \"\"\"Wave-U-Net Generator per separazione audio\"\"\"\n",
    "    def __init__(self, input_channels=2, output_channels=1, base_channels=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder (downsampling)\n",
    "        self.enc1 = WaveUNetBlock(input_channels, base_channels, kernel_size=15)\n",
    "        self.enc2 = WaveUNetBlock(base_channels, base_channels*2, stride=2)\n",
    "        self.enc3 = WaveUNetBlock(base_channels*2, base_channels*4, stride=2)\n",
    "        self.enc4 = WaveUNetBlock(base_channels*4, base_channels*8, stride=2)\n",
    "        self.enc5 = WaveUNetBlock(base_channels*8, base_channels*16, stride=2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = WaveUNetBlock(base_channels*16, base_channels*16, kernel_size=15)\n",
    "        \n",
    "        # Decoder (upsampling)\n",
    "        self.dec5 = nn.ConvTranspose1d(base_channels*16, base_channels*8, \n",
    "                                      kernel_size=4, stride=2, padding=1)\n",
    "        self.dec5_conv = WaveUNetBlock(base_channels*16, base_channels*8)\n",
    "        \n",
    "        self.dec4 = nn.ConvTranspose1d(base_channels*8, base_channels*4, \n",
    "                                      kernel_size=4, stride=2, padding=1)\n",
    "        self.dec4_conv = WaveUNetBlock(base_channels*8, base_channels*4)\n",
    "        \n",
    "        self.dec3 = nn.ConvTranspose1d(base_channels*4, base_channels*2, \n",
    "                                      kernel_size=4, stride=2, padding=1)\n",
    "        self.dec3_conv = WaveUNetBlock(base_channels*4, base_channels*2)\n",
    "        \n",
    "        self.dec2 = nn.ConvTranspose1d(base_channels*2, base_channels, \n",
    "                                      kernel_size=4, stride=2, padding=1)\n",
    "        self.dec2_conv = WaveUNetBlock(base_channels*2, base_channels)\n",
    "        \n",
    "        # Output layer\n",
    "        self.final_conv = nn.Conv1d(base_channels, output_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, mixture, source_b):\n",
    "        # Input: concatena mixture e source_b\n",
    "        x = torch.cat([mixture, source_b], dim=1)\n",
    "        \n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        e4 = self.enc4(e3)\n",
    "        e5 = self.enc5(e4)\n",
    "        \n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(e5)\n",
    "        \n",
    "        # Decoder con skip connections\n",
    "        d5 = self.dec5(b)\n",
    "        d5 = torch.cat([d5, e4], dim=1)\n",
    "        d5 = self.dec5_conv(d5)\n",
    "        \n",
    "        d4 = self.dec4(d5)\n",
    "        d4 = torch.cat([d4, e3], dim=1)\n",
    "        d4 = self.dec4_conv(d4)\n",
    "        \n",
    "        d3 = self.dec3(d4)\n",
    "        d3 = torch.cat([d3, e2], dim=1)\n",
    "        d3 = self.dec3_conv(d3)\n",
    "        \n",
    "        d2 = self.dec2(d3)\n",
    "        d2 = torch.cat([d2, e1], dim=1)\n",
    "        d2 = self.dec2_conv(d2)\n",
    "        \n",
    "        # Output\n",
    "        output = torch.tanh(self.final_conv(d2))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc49e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AudioDiscriminator(nn.Module):\n",
    "    \"\"\"Discriminatore per audio 1D\"\"\"\n",
    "    def __init__(self, input_channels=1, base_channels=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Conv1d(input_channels, base_channels, kernel_size=15, stride=2, padding=7),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Layer 2  \n",
    "            nn.Conv1d(base_channels, base_channels*2, kernel_size=15, stride=2, padding=7),\n",
    "            nn.BatchNorm1d(base_channels*2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Layer 3\n",
    "            nn.Conv1d(base_channels*2, base_channels*4, kernel_size=15, stride=2, padding=7),\n",
    "            nn.BatchNorm1d(base_channels*4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Layer 4\n",
    "            nn.Conv1d(base_channels*4, base_channels*8, kernel_size=15, stride=2, padding=7),\n",
    "            nn.BatchNorm1d(base_channels*8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Layer 5\n",
    "            nn.Conv1d(base_channels*8, base_channels*16, kernel_size=15, stride=2, padding=7),\n",
    "            nn.BatchNorm1d(base_channels*16),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        \n",
    "        # Global average pooling + classificazione\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(base_channels*16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.layers(x)\n",
    "        output = self.classifier(features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2397d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pydub_to_array(audio: AudioSegment) -> (np.ndarray, int):\n",
    "    return np.array(audio.get_array_of_samples(), dtype=np.float32).reshape((audio.channels, -1)) / (\n",
    "            1 << (8 * audio.sample_width - 1)), audio.frame_rate\n",
    "\n",
    "def array_to_pydub(audio_np_array: np.ndarray, sample_rate: int = 16000, sample_width: int = 2, channels: int = 1) -> AudioSegment:\n",
    "    return AudioSegment((audio_np_array * (2 ** (8 * sample_width - 1))).astype(np.int16).tobytes(),\n",
    "                        frame_rate=sample_rate, sample_width=sample_width, channels=channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ea51c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_csv = \"train_dataset_wav.csv\"\n",
    "validation_csv = \"val_dataset_wav.csv\"\n",
    "test_csv = \"test_dataset_wav.csv\"\n",
    "\n",
    "# train_csv = 'pezz.csv'\n",
    "# validation_csv = 'pezz.csv'\n",
    "# test_csv = 'pezz.csv'\n",
    "\n",
    "train = AudioDataset(csv_file=train_csv)\n",
    "validation = AudioDataset(csv_file=validation_csv)\n",
    "test = AudioDataset(csv_file=test_csv)\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15086de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_event_separator(model,\n",
    "                          train_loader,\n",
    "                          val_loader=None,\n",
    "                          *,\n",
    "                          device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                          num_epochs: int = 50,\n",
    "                          lr: float = 1e-4,\n",
    "                          alpha: float = 1.0,\n",
    "                          beta: float = 0.5,\n",
    "                          scheduler=None,\n",
    "                          save_path: str = \"best_event_sep.pth\"):\n",
    "\n",
    "    device = torch.device(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    l1_loss = nn.L1Loss()\n",
    "\n",
    "    history = {\"train\": [], \"val\": []}\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "\n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [train]\", unit=\"batch\") as bar:\n",
    "            for mixture, source_b in bar:\n",
    "                mixture = mixture.to(device)\n",
    "                source_b = source_b.to(device)\n",
    "\n",
    "                # Calcola source_a come residuo\n",
    "                source_a = mixture - source_b\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Il modello predice source_a a partire da (mixture, source_b)\n",
    "                pred_source_a = model(mixture, source_b)\n",
    "\n",
    "                # Loss di ricostruzione: pred + source_b ≈ mixture\n",
    "                loss_rec = l1_loss(pred_source_a + source_b, mixture)\n",
    "\n",
    "                # Loss sull’evento: pred ≈ source_a\n",
    "                loss_evt = l1_loss(pred_source_a, source_a)\n",
    "\n",
    "                loss = alpha * loss_rec + beta * loss_evt\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_train_loss += loss.item()\n",
    "                bar.set_postfix(loss=running_train_loss / (bar.n + 1))\n",
    "\n",
    "        epoch_train_loss = running_train_loss / len(train_loader)\n",
    "        history[\"train\"].append(epoch_train_loss)\n",
    "\n",
    "        # -----------------------\n",
    "        # VALIDATION\n",
    "        # -----------------------\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            running_val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for mixture, source_b in val_loader:\n",
    "                    mixture = mixture.to(device)\n",
    "                    source_b = source_b.to(device)\n",
    "                    source_a = mixture - source_b\n",
    "\n",
    "                    pred_source_a = model(mixture, source_b)\n",
    "                    loss_rec = l1_loss(pred_source_a + source_b, mixture)\n",
    "                    loss_evt = l1_loss(pred_source_a, source_a)\n",
    "                    loss = alpha * loss_rec + beta * loss_evt\n",
    "\n",
    "                    running_val_loss += loss.item()\n",
    "\n",
    "            epoch_val_loss = running_val_loss / len(val_loader)\n",
    "            history[\"val\"].append(epoch_val_loss)\n",
    "\n",
    "            print(f\"\\nEpoch {epoch}/{num_epochs} → train: {epoch_train_loss:.4f} | val: {epoch_val_loss:.4f}\")\n",
    "        else:\n",
    "            history[\"val\"].append(float(\"nan\"))\n",
    "            print(f\"\\nEpoch {epoch}/{num_epochs} → train: {epoch_train_loss:.4f}\")\n",
    "\n",
    "        # Scheduler\n",
    "        if scheduler is not None:\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(epoch_val_loss if val_loader is not None else epoch_train_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "        # Checkpoint\n",
    "        if val_loader is not None and epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"→ Best model saved to '{save_path}' (val loss {best_val_loss:.4f})\")\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82fc0c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_source_a(model, mixture, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Estrae source_a da una mixture senza source_b.\n",
    "    source_b viene impostato a zero.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(device)\n",
    "    mixture = mixture.to(device)\n",
    "\n",
    "    # Crea source_b dummy con shape corretta\n",
    "    source_b_dummy = torch.zeros_like(mixture[:, :1])  # Se mono\n",
    "    with torch.no_grad():\n",
    "        pred_source_a = model(mixture, source_b_dummy)\n",
    "    return pred_source_a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a3bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WaveUNetGenerator(input_channels=2, output_channels=1)\n",
    "\n",
    "history = train_event_separator(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader=validation_loader,\n",
    "    num_epochs=20,\n",
    "    lr=1e-4,\n",
    "    alpha=1.0,\n",
    "    beta=0.5,\n",
    "    save_path=\"best_model.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "67de2703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\Matteo\\AppData\\Local\\Temp\\ipykernel_8520\\1839432641.py:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  mixture = AudioSegment.from_file(\"audio_sources\\dataset_toy\\mix_2\\mixture.wav\").set_channels(1).set_frame_rate(16000)\n"
     ]
    }
   ],
   "source": [
    "mixture = AudioSegment.from_file(\"audio_sources\\dataset_toy\\mix_2\\mixture.wav\").set_channels(1).set_frame_rate(16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93ce2057",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixture_array, _ = pydub_to_array(mixture)\n",
    "mixture_tensor = torch.tensor(mixture_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0f478d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 160000])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixture_tensor.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88e8f31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica modello\n",
    "model = WaveUNetGenerator(input_channels=2, output_channels=1)\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Assumiamo mixture_tensor con shape (1, T) o (T,)\n",
    "if mixture_tensor.dim() == 1:\n",
    "    mixture_tensor = mixture_tensor.unsqueeze(0).unsqueeze(0)  # (1, 1, T)\n",
    "elif mixture_tensor.dim() == 2:\n",
    "    mixture_tensor = mixture_tensor.unsqueeze(0)  # (1, C, T)\n",
    "\n",
    "# Inferenza\n",
    "predicted_source_a = infer_source_a(model, mixture_tensor)\n",
    "\n",
    "# Salvataggio - rimuovo dimensione batch (0)\n",
    "torchaudio.save(\"predicted2.wav\", predicted_source_a.squeeze(0).cpu(), _)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3548398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795ad5eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
