{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a46873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8af52073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_signals(mix, isolated, max_offset=1000):\n",
    "    \"\"\"Trova l'offset migliore per allineare i segnali\"\"\"\n",
    "    correlation = signal.correlate(mix, isolated, mode='full')\n",
    "    offset = np.argmax(correlation) - len(isolated) + 1\n",
    "    return max(-max_offset, min(max_offset, offset))\n",
    "\n",
    "def extract_signal(mix_file, isolated_file, output_file):\n",
    "    # Carica i file\n",
    "    sr_mix, mix = wavfile.read(mix_file)\n",
    "    sr_iso, isolated = wavfile.read(isolated_file)\n",
    "    \n",
    "    # Converti in float per calcoli precisi\n",
    "    mix = mix.astype(np.float64)\n",
    "    isolated = isolated.astype(np.float64)\n",
    "    \n",
    "    # Trova l'allineamento migliore\n",
    "    offset = align_signals(mix, isolated)\n",
    "    \n",
    "    # Applica l'offset\n",
    "    if offset > 0:\n",
    "        isolated = np.pad(isolated, (offset, 0))[:len(mix)]\n",
    "    elif offset < 0:\n",
    "        isolated = isolated[-offset:len(mix)-offset]\n",
    "    \n",
    "    # Equalizza lunghezze\n",
    "    min_len = min(len(mix), len(isolated))\n",
    "    mix = mix[:min_len]\n",
    "    isolated = isolated[:min_len]\n",
    "    \n",
    "    # Estrai il segnale\n",
    "    extracted = mix - isolated\n",
    "    \n",
    "    # Normalizza\n",
    "    if np.max(np.abs(extracted)) > 0:\n",
    "        extracted = extracted / np.max(np.abs(extracted)) * 32767\n",
    "    \n",
    "    extracted = np.clip(extracted, -32768, 32767).astype(np.int16)\n",
    "    \n",
    "    # Salva\n",
    "    wavfile.write(output_file, sr_mix, extracted)\n",
    "    \n",
    "    return extracted\n",
    "\n",
    "# Uso\n",
    "extracted_a = extract_signal('audio_sources/dataset_toy/mix_3/mixture.wav', 'audio_sources/dataset_toy/mix_3/background.wav', 'extracted_a.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e85f12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializza modelli\n",
    "generator = WaveUNetGenerator(input_channels=2, output_channels=1).to(device)\n",
    "discriminator = AudioDiscriminator(input_channels=1).to(device)\n",
    "\n",
    "# Ottimizzatori\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=1e-4)\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-4)\n",
    "\n",
    "# Loss function\n",
    "bce_loss = nn.BCELoss()\n",
    "l1_loss = nn.L1Loss()\n",
    "\n",
    "# Parametro bilanciamento tra perdita avversaria e di ricostruzione\n",
    "lambda_adv = 0.001\n",
    "\n",
    "n_epochs = 20\n",
    "\n",
    "best_g_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    running_g_loss = 0.0\n",
    "    running_d_loss = 0.0\n",
    "\n",
    "    for mixture, background in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\"):\n",
    "        mixture = mixture.to(device)         # (B, 1, T)\n",
    "        background = background.to(device)   # (B, 1, T)\n",
    "\n",
    "        # === Train Generator ===\n",
    "        predicted_a = generator(mixture, background)\n",
    "\n",
    "        # Discriminator stima \"falso\" su predicted\n",
    "        d_fake_pred = discriminator(predicted_a)\n",
    "        g_adv_loss = bce_loss(d_fake_pred, torch.ones_like(d_fake_pred))\n",
    "\n",
    "        # Loss di ricostruzione: confronta predizione con target reale\n",
    "        real_a = (mixture - background).detach()  # ground truth\n",
    "        g_recon_loss = l1_loss(predicted_a, real_a)\n",
    "\n",
    "        g_loss = g_recon_loss + lambda_adv * g_adv_loss\n",
    "\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # === Train Discriminator ===\n",
    "        with torch.no_grad():\n",
    "            fake_a_detached = predicted_a.detach()\n",
    "        d_real_pred = discriminator(real_a)\n",
    "        d_fake_pred = discriminator(fake_a_detached)\n",
    "\n",
    "        d_real_loss = bce_loss(d_real_pred, torch.ones_like(d_real_pred))\n",
    "        d_fake_loss = bce_loss(d_fake_pred, torch.zeros_like(d_fake_pred))\n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        running_g_loss += g_loss.item()\n",
    "        running_d_loss += d_loss.item()\n",
    "\n",
    "    avg_g_loss = running_g_loss / len(train_loader)\n",
    "    avg_d_loss = running_d_loss / len(train_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} | G_loss: {avg_g_loss:.4f} | D_loss: {avg_d_loss:.4f}\")\n",
    "   # torch.save(generator.state_dict(), f\"generator_epoch_{epoch+1}.pth\")\n",
    "   # torch.save(discriminator.state_dict(), f\"discriminator_epoch_{epoch+1}.pth\")\n",
    "    if avg_g_loss < best_g_loss:\n",
    "        best_g_loss = avg_g_loss\n",
    "        torch.save(generator.state_dict(), \"best_generator.pth\")\n",
    "        torch.save(discriminator.state_dict(), \"best_discriminator.pth\")\n",
    "        print(f\"✅ Saved best models at epoch {epoch+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06eab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class UnsupervisedSeparationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Modello completamente unsupervised per audio source separation\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels=1, base_channels=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder condiviso\n",
    "        self.shared_encoder = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, base_channels, 15, 2, 7),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(base_channels, base_channels*2, 15, 2, 7),\n",
    "            nn.BatchNorm1d(base_channels*2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(base_channels*2, base_channels*4, 15, 2, 7),\n",
    "            nn.BatchNorm1d(base_channels*4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(base_channels*4, base_channels*8, 15, 2, 7),\n",
    "            nn.BatchNorm1d(base_channels*8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        \n",
    "        # Decoder separati per ogni sorgente\n",
    "        self.decoder_a = self._build_decoder(base_channels*8, base_channels)\n",
    "        self.decoder_b = self._build_decoder(base_channels*8, base_channels)\n",
    "        \n",
    "        # Output layers\n",
    "        self.output_a = nn.Conv1d(base_channels, 1, 1)\n",
    "        self.output_b = nn.Conv1d(base_channels, 1, 1)\n",
    "        \n",
    "    def _build_decoder(self, in_channels, base_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels, base_channels*4, 4, 2, 1),\n",
    "            nn.BatchNorm1d(base_channels*4),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(base_channels*4, base_channels*2, 4, 2, 1),\n",
    "            nn.BatchNorm1d(base_channels*2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(base_channels*2, base_channels, 4, 2, 1),\n",
    "            nn.BatchNorm1d(base_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(base_channels, base_channels, 4, 2, 1),\n",
    "            nn.BatchNorm1d(base_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, mixture):\n",
    "        # Shared encoding\n",
    "        encoded = self.shared_encoder(mixture)\n",
    "        \n",
    "        # Separate decoding\n",
    "        decoded_a = self.decoder_a(encoded)\n",
    "        decoded_b = self.decoder_b(encoded)\n",
    "        \n",
    "        # Output\n",
    "        source_a = torch.tanh(self.output_a(decoded_a))\n",
    "        source_b = torch.tanh(self.output_b(decoded_b))\n",
    "        \n",
    "        return source_a, source_b\n",
    "\n",
    "class UnsupervisedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Loss function completamente unsupervised\n",
    "    \"\"\"\n",
    "    def __init__(self, lambda_recon=1.0, lambda_indep=0.1, lambda_sparse=0.01, \n",
    "                 lambda_spectral=0.1, lambda_smooth=0.05):\n",
    "        super().__init__()\n",
    "        self.lambda_recon = lambda_recon\n",
    "        self.lambda_indep = lambda_indep\n",
    "        self.lambda_sparse = lambda_sparse\n",
    "        self.lambda_spectral = lambda_spectral\n",
    "        self.lambda_smooth = lambda_smooth\n",
    "    \n",
    "    def forward(self, source_a, source_b, mixture):\n",
    "        # 1. RECONSTRUCTION LOSS\n",
    "        # La somma delle sorgenti deve ricostruire il mixture\n",
    "        reconstructed = source_a + source_b\n",
    "        recon_loss = F.mse_loss(reconstructed, mixture)\n",
    "        \n",
    "        # 2. INDEPENDENCE LOSS\n",
    "        # Le sorgenti devono essere statisticamente indipendenti\n",
    "        # Minimizza la correlazione crociata\n",
    "        independence_loss = self._independence_constraint(source_a, source_b)\n",
    "        \n",
    "        # 3. SPARSITY LOSS\n",
    "        # Incoraggia sorgenti sparse (principio di parsimonia)\n",
    "        sparsity_loss = self._sparsity_constraint(source_a, source_b)\n",
    "        \n",
    "        # 4. SPECTRAL DIVERSITY LOSS\n",
    "        # Le sorgenti devono essere diverse nel dominio delle frequenze\n",
    "        spectral_loss = self._spectral_diversity(source_a, source_b)\n",
    "        \n",
    "        # 5. SMOOTHNESS LOSS\n",
    "        # Incoraggia continuità temporale\n",
    "        smoothness_loss = self._smoothness_constraint(source_a, source_b)\n",
    "        \n",
    "        total_loss = (self.lambda_recon * recon_loss + \n",
    "                     self.lambda_indep * independence_loss +\n",
    "                     self.lambda_sparse * sparsity_loss +\n",
    "                     self.lambda_spectral * spectral_loss +\n",
    "                     self.lambda_smooth * smoothness_loss)\n",
    "        \n",
    "        return {\n",
    "            'total': total_loss,\n",
    "            'reconstruction': recon_loss,\n",
    "            'independence': independence_loss,\n",
    "            'sparsity': sparsity_loss,\n",
    "            'spectral': spectral_loss,\n",
    "            'smoothness': smoothness_loss\n",
    "        }\n",
    "    \n",
    "    def _independence_constraint(self, source_a, source_b):\n",
    "        \"\"\"\n",
    "        Minimizza la correlazione tra le sorgenti\n",
    "        \"\"\"\n",
    "        # Normalizza le sorgenti\n",
    "        a_norm = F.normalize(source_a.flatten(1), dim=1)\n",
    "        b_norm = F.normalize(source_b.flatten(1), dim=1)\n",
    "        \n",
    "        # Correlazione crociata\n",
    "        correlation = torch.mean(torch.sum(a_norm * b_norm, dim=1))\n",
    "        \n",
    "        return torch.abs(correlation)\n",
    "    \n",
    "    def _sparsity_constraint(self, source_a, source_b):\n",
    "        \"\"\"\n",
    "        Incoraggia sparsità nelle sorgenti (L1 norm)\n",
    "        \"\"\"\n",
    "        return torch.mean(torch.abs(source_a)) + torch.mean(torch.abs(source_b))\n",
    "    \n",
    "    def _spectral_diversity(self, source_a, source_b):\n",
    "        \"\"\"\n",
    "        Le sorgenti devono essere diverse nel dominio spettrale\n",
    "        \"\"\"\n",
    "        # FFT delle sorgenti\n",
    "        fft_a = torch.fft.fft(source_a, dim=-1)\n",
    "        fft_b = torch.fft.fft(source_b, dim=-1)\n",
    "        \n",
    "        # Magnitude spectrum\n",
    "        mag_a = torch.abs(fft_a)\n",
    "        mag_b = torch.abs(fft_b)\n",
    "        \n",
    "        # Minimizza la correlazione spettrale\n",
    "        spectral_corr = F.cosine_similarity(mag_a.flatten(1), mag_b.flatten(1), dim=1)\n",
    "        \n",
    "        return torch.mean(torch.abs(spectral_corr))\n",
    "    \n",
    "    def _smoothness_constraint(self, source_a, source_b):\n",
    "        \"\"\"\n",
    "        Incoraggia continuità temporale (TV regularization)\n",
    "        \"\"\"\n",
    "        # Total Variation loss\n",
    "        tv_a = torch.mean(torch.abs(source_a[:, :, 1:] - source_a[:, :, :-1]))\n",
    "        tv_b = torch.mean(torch.abs(source_b[:, :, 1:] - source_b[:, :, :-1]))\n",
    "        \n",
    "        return tv_a + tv_b\n",
    "\n",
    "class UnsupervisedTrainer:\n",
    "    \"\"\"\n",
    "    Trainer per il modello unsupervised\n",
    "    \"\"\"\n",
    "    def __init__(self, model, loss_fn, optimizer, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        \n",
    "    def train_epoch(self, dataloader):\n",
    "        self.model.train()\n",
    "        total_losses = {}\n",
    "        \n",
    "        for batch_idx, (mixture, _) in enumerate(dataloader):\n",
    "            mixture = mixture.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            source_a, source_b = self.model(mixture)\n",
    "            \n",
    "            # Compute losses\n",
    "            losses = self.loss_fn(source_a, source_b, mixture)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            losses['total'].backward()\n",
    "            \n",
    "            # Gradient clipping per stabilità\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Accumula losses\n",
    "            for key, value in losses.items():\n",
    "                if key not in total_losses:\n",
    "                    total_losses[key] = 0\n",
    "                total_losses[key] += value.item()\n",
    "        \n",
    "        # Media delle losses\n",
    "        for key in total_losses:\n",
    "            total_losses[key] /= len(dataloader)\n",
    "            \n",
    "        return total_losses\n",
    "    \n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        total_losses = {}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for mixture, _ in dataloader:\n",
    "                mixture = mixture.to(self.device)\n",
    "                source_a, source_b = self.model(mixture)\n",
    "                losses = self.loss_fn(source_a, source_b, mixture)\n",
    "                \n",
    "                for key, value in losses.items():\n",
    "                    if key not in total_losses:\n",
    "                        total_losses[key] = 0\n",
    "                    total_losses[key] += value.item()\n",
    "        \n",
    "        for key in total_losses:\n",
    "            total_losses[key] /= len(dataloader)\n",
    "            \n",
    "        return total_losses\n",
    "\n",
    "# Esempio di utilizzo\n",
    "def train_unsupervised_model():\n",
    "    \"\"\"\n",
    "    Esempio di training completamente unsupervised\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Modello\n",
    "    model = UnsupervisedSeparationModel()\n",
    "    \n",
    "    # Loss function\n",
    "    loss_fn = UnsupervisedLoss(\n",
    "        lambda_recon=1.0,    # Ricostruzione è la più importante\n",
    "        lambda_indep=0.1,    # Indipendenza\n",
    "        lambda_sparse=0.01,  # Sparsità\n",
    "        lambda_spectral=0.1, # Diversità spettrale\n",
    "        lambda_smooth=0.05   # Smoothness\n",
    "    )\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999))\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = UnsupervisedTrainer(model, loss_fn, optimizer, device)\n",
    "    \n",
    "    return trainer, model\n",
    "\n",
    "# Funzione di analisi dei risultati\n",
    "def analyze_separation_quality(source_a, source_b, mixture):\n",
    "    \"\"\"\n",
    "    Analizza la qualità della separazione senza ground truth\n",
    "    \"\"\"\n",
    "    # 1. Reconstruction error\n",
    "    recon_error = F.mse_loss(source_a + source_b, mixture)\n",
    "    \n",
    "    # 2. Source independence\n",
    "    correlation = F.cosine_similarity(source_a.flatten(), source_b.flatten(), dim=0)\n",
    "    \n",
    "    # 3. Energy distribution\n",
    "    energy_a = torch.mean(source_a**2)\n",
    "    energy_b = torch.mean(source_b**2)\n",
    "    energy_ratio = min(energy_a, energy_b) / max(energy_a, energy_b)\n",
    "    \n",
    "    # 4. Spectral diversity\n",
    "    fft_a = torch.abs(torch.fft.fft(source_a))\n",
    "    fft_b = torch.abs(torch.fft.fft(source_b))\n",
    "    spectral_sim = F.cosine_similarity(fft_a.flatten(), fft_b.flatten(), dim=0)\n",
    "    \n",
    "    return {\n",
    "        'reconstruction_error': recon_error.item(),\n",
    "        'correlation': correlation.item(),\n",
    "        'energy_ratio': energy_ratio.item(),\n",
    "        'spectral_similarity': spectral_sim.item()\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
