{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-11T08:44:24.863752Z",
     "start_time": "2025-08-11T08:44:08.962579Z"
    }
   },
   "source": [
    "import warnings\n",
    "from torch import nn, optim\n",
    "\n",
    "from data_loader import *\n",
    "from trainer import trainer_\n",
    "from aed_models import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T08:45:38.540967Z",
     "start_time": "2025-08-11T08:45:28.223537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "h = 128\n",
    "learning_rate = 1e-3\n",
    "epochs = 15\n",
    "batch_size = 256\n",
    "batch_size_2 = 64\n",
    "\n",
    "ae_model = NS_1().to(device)\n",
    "# clf_model = Wavegram_AttentionMap(h=h, lr=learning_rate).to(device)\n",
    "optimizer = optim.AdamW(ae_model.parameters(), lr=1e-3,  weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()"
   ],
   "id": "f4a608dd87600ae9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T08:46:09.996207Z",
     "start_time": "2025-08-11T08:46:08.795240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### === UNSUPERVISED === ###\n",
    "ae_dataset = AudioDataset(\n",
    "    audio_dir=\"audio_sources/dataset/background_nash/0_normal_1\",\n",
    "    supervised=False)\n",
    "\n",
    "# Split unsupervised\n",
    "train_size = int(0.75 * len(ae_dataset))\n",
    "val_size = int(0.15 * len(ae_dataset))\n",
    "test_size = len(ae_dataset) - train_size - val_size\n",
    "ae_train, ae_val, ae_test = random_split(ae_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "ae_train_loader = DataLoader(ae_train, batch_size=batch_size, shuffle=True)\n",
    "ae_val_loader = DataLoader(ae_val, batch_size=batch_size, shuffle=False)\n",
    "ae_test_loader = DataLoader(ae_test, batch_size=batch_size, shuffle=False)"
   ],
   "id": "d648ce841fda0d4d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T08:46:11.886505Z",
     "start_time": "2025-08-11T08:46:11.883468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### === INFO === ###\n",
    "print(\"Ready!\")\n",
    "print(f\" - AE: {len(ae_dataset)} samples\")"
   ],
   "id": "c2ee7f863f3b9025",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready!\n",
      " - AE: 958580 samples\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainer_(model=ae_model, train_loader=ae_train_loader, val_loader=ae_val_loader,\n",
    "         criterion=criterion, optimizer=optimizer, device=device, num_epochs=15,\n",
    "         results_dir=\"results_1s\", csv_name=\"training_log_1s.csv\", model_name=\"best_model_1s.pth\")"
   ],
   "id": "f0d6f830fa4be4f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sample_rate = 16000\n",
    "duration = 5\n",
    "seg_len = sample_rate * duration\n",
    "threshold = 0.0020"
   ],
   "id": "ed9b659faf461fa8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Parametri globali\n",
    "sample_rate = 16000\n",
    "duration = 5\n",
    "seg_len = sample_rate * duration  # 160000\n",
    "threshold = 0.0020\n",
    "severe_threshold = 0.0040"
   ],
   "id": "60813e29d3f8b0ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === DETECTOR === #\n",
    "def load_audio(filepath, target_length=seg_len):\n",
    "    waveform, sr = torchaudio.load(filepath)\n",
    "    if sr != sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # Mono\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    original_length = waveform.shape[1]  # salviamo la lunghezza reale\n",
    "\n",
    "    # Padding a 10s (senza taglio se >10s)\n",
    "    if original_length < target_length:\n",
    "        pad_len = target_length - original_length\n",
    "        waveform = F.pad(waveform, (0, pad_len))\n",
    "    else:\n",
    "        waveform = waveform[:, :target_length]\n",
    "        original_length = target_length  # limitiamo a max 10s\n",
    "\n",
    "    return waveform.squeeze(0), original_length  # [160000], valore reale\n",
    "\n",
    "\n",
    "def run_inference(model, audio_tensor, original_length, threshold=threshold):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    model.transform_tf = model.transform_tf.to(device)\n",
    "\n",
    "    audio_tensor = audio_tensor.unsqueeze(0).to(device)  # [1, 160000]\n",
    "    feature_vector = model.preprocessing(audio_tensor)  # [1, T, 640]\n",
    "\n",
    "    # Calcola quanti frame sono reali (non padding)\n",
    "    real_t_bins = 1 + (original_length // model.transform_tf.hop_length)\n",
    "    real_vector_array_size = real_t_bins - model.frames + 1\n",
    "\n",
    "    # Ricostruzione\n",
    "    with torch.no_grad():\n",
    "        encoded = model.encoder(feature_vector)\n",
    "        bottleneck = model.bottleneck(encoded)\n",
    "        reconstructed = model.decoder(bottleneck)\n",
    "\n",
    "    # Taglia alla durata reale\n",
    "    feature_vector = feature_vector[:, :real_vector_array_size, :]\n",
    "    reconstructed = reconstructed[:, :real_vector_array_size, :]\n",
    "\n",
    "    # Soglie\n",
    "\n",
    "    # Errore globale\n",
    "    loss = F.mse_loss(reconstructed, feature_vector).item()\n",
    "\n",
    "    # Classificazione in base alla soglia\n",
    "    if loss > severe_threshold:\n",
    "        result = \"Anomalia Grave\"\n",
    "    elif loss > threshold:\n",
    "        result = \"Anomalo\"\n",
    "    else:\n",
    "        result = \"Normale\"\n",
    "\n",
    "    print(f\"Anomaly Score (MSE): {loss:.6f} → {result}\")\n",
    "\n",
    "\n",
    "    # === Visualizzazione === #\n",
    "    with torch.no_grad():\n",
    "        reconstruction_error = torch.mean((feature_vector - reconstructed) ** 2, dim=2).squeeze()  # [T]\n",
    "        diff = (feature_vector - reconstructed).abs().squeeze()  # [T, 640]\n",
    "        # diff_mel = diff.view(-1, 5, 128).mean(dim=1)  # [T, 128]\n",
    "        diff_mel = diff.reshape(-1, 128)\n",
    "\n",
    "    # --- Plot 1D ---\n",
    "    # --- Plot 1D (con threshold e tempo in secondi) ---\n",
    "    reconstruction_error_np = reconstruction_error.cpu().numpy()\n",
    "    num_frames = reconstruction_error_np.shape[0]\n",
    "    hop_length = model.transform_tf.hop_length\n",
    "    frame_times = torch.arange(num_frames) * hop_length / sample_rate  # tempo in secondi\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(frame_times, reconstruction_error_np, label=\"Errore di ricostruzione\")\n",
    "    plt.axhline(y=threshold, color='red', linestyle='--', label=f\"Soglia ({threshold:.4f})\")\n",
    "    plt.title(\"Errore di Ricostruzione per Frame\")\n",
    "    plt.xlabel(\"Tempo [s]\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Plot 2D ---\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(diff_mel.cpu().T, aspect=\"auto\", origin=\"lower\", cmap=\"viridis\",\n",
    "               extent=[frame_times[0].item(), frame_times[-1].item(), 0, 128])\n",
    "    plt.title(\"Errore di Ricostruzione (Tempo × Frequenza)\")\n",
    "    plt.xlabel(\"Tempo [s]\")\n",
    "    plt.ylabel(\"Mel Frequency Bin\")\n",
    "    plt.colorbar(label=\"|Errore|\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    return feature_vector.cpu(), reconstructed.cpu(), loss\n"
   ],
   "id": "c7afdcda8e804c14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === RUN === #\n",
    "audio_path = \"test/200501.flac\"\n",
    "model_path = \"results_5s/best_model_5s.pth\"\n",
    "\n",
    "# Carica modello\n",
    "model = NS_5()\n",
    "model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
    "\n",
    "# Carica audio e fai inferenza\n",
    "waveform, original_length = load_audio(audio_path)\n",
    "original, reconstructed, loss = run_inference(model, waveform, original_length)\n"
   ],
   "id": "90f405ff03864038",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "print(type(state_dict))  # Se è <class 'dict'> con 'state_dict', è un modello intero\n"
   ],
   "id": "643b92da5c89c691",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for k in state_dict.keys():\n",
    "    print(k)\n"
   ],
   "id": "3f135c2bdf7fc3c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "605732bbfbd53702",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
