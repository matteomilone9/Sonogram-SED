{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-11T08:28:16.141199Z",
     "start_time": "2025-06-11T08:28:10.636798Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "from itertools import cycle"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T08:28:16.156827Z",
     "start_time": "2025-06-11T08:28:16.141199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AudioDatasetMixture(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, target_duration=10000, target_sample_rate=16000,\n",
    "                 target_channels=1):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.target_duration = target_duration\n",
    "        self.target_channels = target_channels\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        mixture_path = os.path.join(self.df.loc[idx, 'mixture'])\n",
    "        mixture_audio = AudioSegment.from_file(mixture_path).set_channels(self.target_channels).set_frame_rate(self.target_sample_rate)\n",
    "        mixture, _ = self._pydub_to_array(mixture_audio)\n",
    "        mixture_tensor = torch.Tensor(mixture)\n",
    "\n",
    "        return mixture_tensor\n",
    "\n",
    "    def _pydub_to_array(self, audio: AudioSegment) -> (np.ndarray, int):\n",
    "        return np.array(audio.get_array_of_samples(), dtype=np.float32).reshape((audio.channels, -1)) / (\n",
    "                1 << (8 * audio.sample_width - 1)), audio.frame_rate"
   ],
   "id": "a68e0d11dab5882",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T08:28:16.166211Z",
     "start_time": "2025-06-11T08:28:16.156827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AudioDatasetBackground(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, target_duration=10000, target_sample_rate=16000,\n",
    "                 target_channels=1):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.target_duration = target_duration\n",
    "        self.target_channels = target_channels\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        background_path = os.path.join(self.df.loc[idx, 'background'])\n",
    "        background_audio = AudioSegment.from_file(background_path).set_channels(self.target_channels).set_frame_rate(self.target_sample_rate)\n",
    "        background, _ = self._pydub_to_array(background_audio)\n",
    "        background_tensor = torch.Tensor(background)\n",
    "\n",
    "        return background_tensor\n",
    "\n",
    "    def _pydub_to_array(self, audio: AudioSegment) -> (np.ndarray, int):\n",
    "        return np.array(audio.get_array_of_samples(), dtype=np.float32).reshape((audio.channels, -1)) / (\n",
    "                1 << (8 * audio.sample_width - 1)), audio.frame_rate"
   ],
   "id": "7671f1c6e56c2861",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T08:28:16.403036Z",
     "start_time": "2025-06-11T08:28:16.387412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pydub_to_array(audio: AudioSegment) -> (np.ndarray, int):\n",
    "    return np.array(audio.get_array_of_samples(), dtype=np.float32).reshape((audio.channels, -1)) / (\n",
    "            1 << (8 * audio.sample_width - 1)), audio.frame_rate\n",
    "\n",
    "def array_to_pydub(audio_np_array: np.ndarray, sample_rate: int = 16000, sample_width: int = 2, channels: int = 1) -> AudioSegment:\n",
    "    return AudioSegment((audio_np_array * (2 ** (8 * sample_width - 1))).astype(np.int16).tobytes(),\n",
    "                        frame_rate=sample_rate, sample_width=sample_width, channels=channels)"
   ],
   "id": "996a946e908b4c1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T08:28:16.765050Z",
     "start_time": "2025-06-11T08:28:16.418661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_csv = \"csv/train_wav.csv\"\n",
    "validation_csv = \"csv/val_wav.csv\"\n",
    "test_csv = \"csv/test_wav.csv\"\n",
    "\n",
    "train_m = AudioDatasetMixture(csv_file=train_csv)\n",
    "validation_m = AudioDatasetMixture(csv_file=validation_csv)\n",
    "test_m = AudioDatasetMixture(csv_file=test_csv)\n",
    "\n",
    "train_b = AudioDatasetBackground(csv_file=train_csv)\n",
    "validation_b = AudioDatasetBackground(csv_file=validation_csv)\n",
    "test_b = AudioDatasetBackground(csv_file=test_csv)\n",
    "\n",
    "batch_size = 18\n",
    "\n",
    "mixture_train_loader = torch.utils.data.DataLoader(train_m, batch_size=batch_size, shuffle=True)\n",
    "mixture_validation_loader = torch.utils.data.DataLoader(validation_m, batch_size=batch_size, shuffle=False)\n",
    "mixture_test_loader = torch.utils.data.DataLoader(test_m, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "background_train_loader = torch.utils.data.DataLoader(train_b, batch_size=batch_size, shuffle=True)\n",
    "background_validation_loader = torch.utils.data.DataLoader(validation_b, batch_size=batch_size, shuffle=False)\n",
    "background_test_loader = torch.utils.data.DataLoader(test_b, batch_size=batch_size, shuffle=False)"
   ],
   "id": "cca2a422d61f4289",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T08:28:16.812137Z",
     "start_time": "2025-06-11T08:28:16.796524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SeparableConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, bias=True, norm=True, activation=True):\n",
    "        super(SeparableConv1d, self).__init__()\n",
    "        self.depthwise = nn.Conv1d(in_channels, in_channels, kernel_size=kernel_size,\n",
    "                                   stride=stride, padding=padding, dilation=dilation,\n",
    "                                   groups=in_channels, bias=bias)\n",
    "        self.pointwise = nn.Conv1d(in_channels, out_channels, kernel_size=1,\n",
    "                                   stride=1, padding=0, bias=bias)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(out_channels) if norm else nn.Identity()\n",
    "        self.act = nn.LeakyReLU(inplace=True) if activation else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x"
   ],
   "id": "1bc0fe0ef5e57c0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T08:28:16.843416Z",
     "start_time": "2025-06-11T08:28:16.827773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LocalSelfAttention1D(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, window_size=31):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, T) → (B, T, C) for attention\n",
    "        x = x.permute(0, 2, 1)\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        out = torch.zeros_like(x)\n",
    "\n",
    "        half_window = self.window_size // 2\n",
    "        for t in range(T):\n",
    "            start = max(0, t - half_window)\n",
    "            end = min(T, t + half_window + 1)\n",
    "\n",
    "            context = x[:, start:end, :]\n",
    "            query = x[:, t:t+1, :]\n",
    "            attn_out, _ = self.attn(query, context, context)\n",
    "            out[:, t:t+1, :] = attn_out\n",
    "\n",
    "        # back to (B, C, T)\n",
    "        return out.permute(0, 2, 1)"
   ],
   "id": "a5cc6afa2ad675f0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T08:28:16.874654Z",
     "start_time": "2025-06-11T08:28:16.859043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### UNet1D con SepConv - Profondità 4 ###\n",
    "class UNet1D(nn.Module):\n",
    "    def __init__(self, input_channels=1, base_channels=32):\n",
    "        super(UNet1D, self).__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = SeparableConv1d(input_channels, base_channels, kernel_size=9, padding=4)\n",
    "        self.enc2 = SeparableConv1d(base_channels, base_channels*2, kernel_size=9, stride=2, padding=4)\n",
    "        self.enc3 = SeparableConv1d(base_channels*2, base_channels*4, kernel_size=9, stride=2, padding=4)\n",
    "        self.enc4 = SeparableConv1d(base_channels*4, base_channels*8, kernel_size=9, stride=2, padding=4)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = SeparableConv1d(base_channels*8, base_channels*8, kernel_size=9, padding=4)\n",
    "        \n",
    "        # Decoder\n",
    "        self.dec4 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(base_channels*8, base_channels*4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(base_channels*4),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(base_channels*8, base_channels*2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(base_channels*2),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(base_channels*4, base_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(base_channels),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.dec1 = nn.Sequential(\n",
    "            SeparableConv1d(base_channels*2, input_channels, kernel_size=9, padding=4, norm=False, activation=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        e4 = self.enc4(e3)\n",
    "        \n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(e4)\n",
    "        \n",
    "        # Decoder\n",
    "        d4 = self.dec4(b)\n",
    "        d4 = torch.cat([d4, e3], dim=1)\n",
    "        d3 = self.dec3(d4)\n",
    "        d3 = torch.cat([d3, e2], dim=1)\n",
    "        d2 = self.dec2(d3)\n",
    "        d2 = torch.cat([d2, e1], dim=1)\n",
    "        out = self.dec1(d2)\n",
    "        \n",
    "        return out"
   ],
   "id": "84adad6e07df4436",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T08:28:16.905904Z",
     "start_time": "2025-06-11T08:28:16.890279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Discriminator1D(nn.Module):\n",
    "    def __init__(self, input_channels=1, base_channels=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, base_channels, kernel_size=7, stride=2, padding=3),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(base_channels, base_channels*2, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm1d(base_channels*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv1d(base_channels*2, base_channels*4, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm1d(base_channels*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv1d(base_channels*4, base_channels*8, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm1d(base_channels*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.final_conv = nn.Conv1d(base_channels*8, 1, kernel_size=3, padding=1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.final_conv(x)\n",
    "        x = self.avgpool(x)\n",
    "        return x.view(-1)"
   ],
   "id": "72834cecb1d98ef4",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T08:28:16.937155Z",
     "start_time": "2025-06-11T08:28:16.921538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # === MODELLI & PARAMETRI ===\n",
    "# generator = UNet1D().to(device)\n",
    "# discriminator = Discriminator1D().to(device)\n",
    "# \n",
    "# num_epochs = 30\n",
    "# adversarial_loss = nn.BCEWithLogitsLoss()\n",
    "# lambda_l1 = 10 \n",
    "# loss_fn = nn.L1Loss()\n",
    "# \n",
    "# # Learning rate bilanciati\n",
    "# optimizer_G = torch.optim.AdamW(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "# optimizer_D = torch.optim.AdamW(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "# \n",
    "# # === FILE E PATH ===\n",
    "# os.makedirs(\"results-3\", exist_ok=True)\n",
    "# csv_path = \"results-3/training_history.csv\"\n",
    "# checkpoint_path = \"results-3/checkpoint.pth\"\n",
    "# \n",
    "# # Definizione delle intestazioni del CSV \n",
    "# fieldnames = ['epoch', 'train_loss_D', 'train_loss_G', 'train_loss_G_GAN', 'train_loss_G_L1', 'val_loss_L1']\n",
    "# \n",
    "# # Inizializza il CSV solo se non esiste\n",
    "# if not os.path.exists(csv_path):\n",
    "#     with open(csv_path, 'w', newline='') as csvfile:\n",
    "#         writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "#         writer.writeheader()\n",
    "# \n",
    "# # === CHECKPOINT: riprendi se esiste ===\n",
    "# start_epoch = 0\n",
    "# best_val_loss = float('inf')\n",
    "# if os.path.exists(checkpoint_path):\n",
    "#     print(\"Loading checkpoint...\")\n",
    "#     checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "#     generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "#     discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "#     optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "#     optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
    "#     start_epoch = checkpoint['epoch'] + 1\n",
    "#     best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "#     print(f\"Resuming training from epoch {start_epoch}\")\n",
    "# \n",
    "# # === TRAINING LOOP ===\n",
    "# for epoch in range(start_epoch, num_epochs):\n",
    "#     generator.train()\n",
    "#     discriminator.train()\n",
    "#     running_loss_D = 0.0\n",
    "#     running_loss_G = 0.0\n",
    "#     running_loss_G_GAN = 0.0\n",
    "#     running_loss_G_L1 = 0.0\n",
    "# \n",
    "#     train_pbar = tqdm(zip(mixture_train_loader, cycle(background_train_loader)),\n",
    "#                       total=len(mixture_train_loader),\n",
    "#                       desc=f'Epoch {epoch+1}/{num_epochs} - Training',\n",
    "#                       leave=False, unit='batch')\n",
    "# \n",
    "#     for mixture, background in train_pbar:\n",
    "#         mixture = mixture.to(device)\n",
    "#         background = background.to(device)\n",
    "#         if mixture.ndim == 2:\n",
    "#             mixture = mixture.unsqueeze(1)\n",
    "#         if background.ndim == 2:\n",
    "#             background = background.unsqueeze(1)\n",
    "# \n",
    "#         # --- 1. Train Discriminator ---\n",
    "#         optimizer_D.zero_grad()\n",
    "#         with torch.no_grad():\n",
    "#             fake_background = generator(mixture).detach()\n",
    "#         real_output = discriminator(background)\n",
    "#         fake_output = discriminator(fake_background)\n",
    "#         real_labels = torch.ones_like(real_output)\n",
    "#         fake_labels = torch.zeros_like(fake_output)\n",
    "#         loss_D_real = adversarial_loss(real_output, real_labels)\n",
    "#         loss_D_fake = adversarial_loss(fake_output, fake_labels)\n",
    "#         loss_D = 0.5 * (loss_D_real + loss_D_fake)\n",
    "#         loss_D.backward()\n",
    "#         optimizer_D.step()\n",
    "# \n",
    "#         # --- 2. Train Generator ---\n",
    "#         optimizer_G.zero_grad()\n",
    "#         fake_background = generator(mixture)\n",
    "#         fake_output = discriminator(fake_background)\n",
    "#         loss_G_GAN = adversarial_loss(fake_output, real_labels)\n",
    "#         loss_G_L1 = F.l1_loss(fake_background, background)\n",
    "#         loss_G = loss_G_GAN + lambda_l1 * loss_G_L1\n",
    "#         loss_G.backward()\n",
    "#         optimizer_G.step()\n",
    "# \n",
    "#         running_loss_D += loss_D.item()\n",
    "#         running_loss_G += loss_G.item()\n",
    "#         running_loss_G_GAN += loss_G_GAN.item()\n",
    "#         running_loss_G_L1 += loss_G_L1.item()\n",
    "# \n",
    "#         train_pbar.set_postfix({\n",
    "#             'D_loss': f'{loss_D.item():.4f}',\n",
    "#             'G_loss': f'{loss_G.item():.4f}',\n",
    "#             'G_GAN': f'{loss_G_GAN.item():.4f}',\n",
    "#             'G_L1': f'{loss_G_L1.item():.4f}'\n",
    "#         })\n",
    "# \n",
    "#     train_pbar.close()\n",
    "# \n",
    "#     avg_loss_D = running_loss_D / len(mixture_train_loader)\n",
    "#     avg_loss_G = running_loss_G / len(mixture_train_loader)\n",
    "#     avg_loss_G_GAN = running_loss_G_GAN / len(mixture_train_loader)\n",
    "#     avg_loss_G_L1 = running_loss_G_L1 / len(mixture_train_loader)\n",
    "# \n",
    "#     # === VALIDATION ===\n",
    "#     generator.eval()\n",
    "#     val_loss_L1 = 0.0\n",
    "# \n",
    "#     val_pbar = tqdm(zip(mixture_validation_loader, cycle(background_validation_loader)),\n",
    "#                     total=len(mixture_validation_loader),\n",
    "#                     desc=f'Epoch {epoch+1}/{num_epochs} - Validation',\n",
    "#                     leave=False, unit='batch')\n",
    "# \n",
    "#     with torch.no_grad():\n",
    "#         for mixture, background in val_pbar:\n",
    "#             mixture = mixture.to(device)\n",
    "#             background = background.to(device)\n",
    "#             if mixture.ndim == 2:\n",
    "#                 mixture = mixture.unsqueeze(1)\n",
    "#             if background.ndim == 2:\n",
    "#                 background = background.unsqueeze(1)\n",
    "# \n",
    "#             pred_background = generator(mixture)\n",
    "#             batch_val_loss = F.l1_loss(pred_background, background).item()\n",
    "#             val_loss_L1 += batch_val_loss\n",
    "# \n",
    "#             val_pbar.set_postfix({'Val_L1': f'{batch_val_loss:.4f}'})\n",
    "# \n",
    "#     val_pbar.close()\n",
    "#     val_loss_L1 /= len(mixture_validation_loader)\n",
    "# \n",
    "#     print(f\"[Epoch {epoch+1}/{num_epochs}] \"\n",
    "#           f\"Train_Loss_D: {avg_loss_D:.4f} | \"\n",
    "#           f\"Train_Loss_G: {avg_loss_G:.4f} | \"\n",
    "#           f\"G_GAN: {avg_loss_G_GAN:.4f} | \"\n",
    "#           f\"G_L1: {avg_loss_G_L1:.4f} | \"\n",
    "#           f\"Val_L1: {val_loss_L1:.4f}\")\n",
    "# \n",
    "#     # === LOG E CSV ===\n",
    "#     with open(csv_path, 'a', newline='') as csvfile:\n",
    "#         writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "#         writer.writerow({\n",
    "#             'epoch': epoch + 1,\n",
    "#             'train_loss_D': avg_loss_D,\n",
    "#             'train_loss_G': avg_loss_G,\n",
    "#             'train_loss_G_GAN': avg_loss_G_GAN,\n",
    "#             'train_loss_G_L1': avg_loss_G_L1,\n",
    "#             'val_loss_L1': val_loss_L1\n",
    "#         })\n",
    "# \n",
    "#     # === CHECKPOINT ===\n",
    "#     checkpoint = {\n",
    "#         'epoch': epoch,\n",
    "#         'generator_state_dict': generator.state_dict(),\n",
    "#         'discriminator_state_dict': discriminator.state_dict(),\n",
    "#         'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "#         'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "#         'best_val_loss': best_val_loss\n",
    "#     }\n",
    "#     torch.save(checkpoint, checkpoint_path)\n",
    "# \n",
    "#     # === BEST MODEL ===\n",
    "#     if val_loss_L1 < best_val_loss:\n",
    "#         best_val_loss = val_loss_L1\n",
    "#         torch.save(generator.state_dict(), \"results-3/best_generator.pth\")\n",
    "#         torch.save(discriminator.state_dict(), \"results-3/best_discriminator.pth\")\n",
    "#         print(f\"New best model saved at epoch {epoch+1} with Val_L1: {val_loss_L1:.4f}\")\n",
    "# \n",
    "# print(f\"\\nTraining completed! Best validation L1 loss: {best_val_loss:.4f}\")\n",
    "# print(f\"Training history saved to: {csv_path}\")\n"
   ],
   "id": "6965a6ebc2fd719f",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-11T08:28:16.968404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === MODELLI & PARAMETRI ===\n",
    "generator = UNet1D().to(device)\n",
    "discriminator = Discriminator1D().to(device)\n",
    "\n",
    "num_epochs = 30\n",
    "adversarial_loss = nn.BCEWithLogitsLoss()\n",
    "lambda_l1 = 10 \n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "# === STRATEGIA 1: Learning rates differenti ===\n",
    "# Discriminatore più lento del generatore\n",
    "optimizer_G = torch.optim.AdamW(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.AdamW(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.999))  # Più lento\n",
    "\n",
    "# === STRATEGIA 2: Frequency training ===\n",
    "# Allena il discriminatore ogni N step del generatore\n",
    "d_train_freq = 2  # Allena D ogni 2 step di G\n",
    "g_steps = 0\n",
    "\n",
    "# === STRATEGIA 3: Soglie dinamiche ===\n",
    "# Se D diventa troppo forte, rallentalo ulteriormente\n",
    "d_accuracy_threshold = 0.8  # Se D è accurato > 80%, rallentalo\n",
    "g_loss_threshold = 2.0      # Se G loss è alta, aiuta G\n",
    "\n",
    "# === FILE E PATH ===\n",
    "os.makedirs(\"results-4\", exist_ok=True)\n",
    "csv_path = \"results-4/training_history.csv\"\n",
    "checkpoint_path = \"results-4/checkpoint.pth\"\n",
    "\n",
    "# Definizione delle intestazioni del CSV \n",
    "fieldnames = ['epoch', 'train_loss_D', 'train_loss_G', 'train_loss_G_GAN', 'train_loss_G_L1', \n",
    "              'val_loss_L1', 'd_accuracy', 'd_train_steps', 'g_train_steps']\n",
    "\n",
    "# Inizializza il CSV solo se non esiste\n",
    "if not os.path.exists(csv_path):\n",
    "    with open(csv_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "# === CHECKPOINT: riprendi se esiste ===\n",
    "start_epoch = 0\n",
    "best_val_loss = float('inf')\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"Loading checkpoint...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "    optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "    optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "    g_steps = checkpoint.get('g_steps', 0)\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")\n",
    "\n",
    "# === TRAINING LOOP ===\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    running_loss_D = 0.0\n",
    "    running_loss_G = 0.0\n",
    "    running_loss_G_GAN = 0.0\n",
    "    running_loss_G_L1 = 0.0\n",
    "    \n",
    "    # Contatori per il bilanciamento\n",
    "    d_train_steps = 0\n",
    "    g_train_steps = 0\n",
    "    d_correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    train_pbar = tqdm(zip(mixture_train_loader, cycle(background_train_loader)),\n",
    "                      total=len(mixture_train_loader),\n",
    "                      desc=f'Epoch {epoch+1}/{num_epochs} - Training',\n",
    "                      leave=False, unit='batch')\n",
    "\n",
    "    for batch_idx, (mixture, background) in enumerate(train_pbar):\n",
    "        mixture = mixture.to(device)\n",
    "        background = background.to(device)\n",
    "        if mixture.ndim == 2:\n",
    "            mixture = mixture.unsqueeze(1)\n",
    "        if background.ndim == 2:\n",
    "            background = background.unsqueeze(1)\n",
    "\n",
    "        # Genera fake samples\n",
    "        with torch.no_grad():\n",
    "            fake_background = generator(mixture)\n",
    "\n",
    "        # === CALCOLA ACCURATEZZA DISCRIMINATORE ===\n",
    "        with torch.no_grad():\n",
    "            real_output = discriminator(background)\n",
    "            fake_output = discriminator(fake_background.detach())\n",
    "            \n",
    "            # Calcola accuratezza (probabilità > 0.5 per real, < 0.5 per fake)\n",
    "            real_preds = torch.sigmoid(real_output) > 0.5\n",
    "            fake_preds = torch.sigmoid(fake_output) < 0.5\n",
    "            \n",
    "            d_correct_predictions += (real_preds.sum() + fake_preds.sum()).item()\n",
    "            total_predictions += real_preds.numel() + fake_preds.numel()\n",
    "\n",
    "        current_d_accuracy = d_correct_predictions / max(total_predictions, 1)\n",
    "\n",
    "        # === DECISIONE: ALLENARE DISCRIMINATORE? ===\n",
    "        should_train_d = True\n",
    "        \n",
    "        # Strategia frequency-based\n",
    "        if g_steps % d_train_freq != 0:\n",
    "            should_train_d = False\n",
    "            \n",
    "        # Strategia threshold-based: se D è troppo accurato, non allenarlo\n",
    "        if current_d_accuracy > d_accuracy_threshold:\n",
    "            should_train_d = False\n",
    "            \n",
    "        # --- TRAIN DISCRIMINATOR (condizionale) ---\n",
    "        if should_train_d:\n",
    "            optimizer_D.zero_grad()\n",
    "            \n",
    "            # Fresh forward pass per il discriminatore\n",
    "            real_output = discriminator(background)\n",
    "            fake_output = discriminator(fake_background.detach())\n",
    "            \n",
    "            real_labels = torch.ones_like(real_output)\n",
    "            fake_labels = torch.zeros_like(fake_output)\n",
    "            \n",
    "            loss_D_real = adversarial_loss(real_output, real_labels)\n",
    "            loss_D_fake = adversarial_loss(fake_output, fake_labels)\n",
    "            loss_D = 0.5 * (loss_D_real + loss_D_fake)\n",
    "            \n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "            d_train_steps += 1\n",
    "        else:\n",
    "            # Se non alleniamo D, usa l'ultima loss per logging\n",
    "            with torch.no_grad():\n",
    "                real_output = discriminator(background)\n",
    "                fake_output = discriminator(fake_background.detach())\n",
    "                real_labels = torch.ones_like(real_output)\n",
    "                fake_labels = torch.zeros_like(fake_output)\n",
    "                loss_D_real = adversarial_loss(real_output, real_labels)\n",
    "                loss_D_fake = adversarial_loss(fake_output, fake_labels)\n",
    "                loss_D = 0.5 * (loss_D_real + loss_D_fake)\n",
    "\n",
    "        # --- TRAIN GENERATOR ---\n",
    "        optimizer_G.zero_grad()\n",
    "        fake_background = generator(mixture)\n",
    "        fake_output = discriminator(fake_background)\n",
    "        \n",
    "        real_labels = torch.ones_like(fake_output)\n",
    "        loss_G_GAN = adversarial_loss(fake_output, real_labels)\n",
    "        loss_G_L1 = F.l1_loss(fake_background, background)\n",
    "        \n",
    "        # Peso dinamico: se G sta soffrendo troppo, riduci il termine adversarial\n",
    "        if loss_G_GAN.item() > g_loss_threshold:\n",
    "            adaptive_lambda = lambda_l1 * 1.5  # Aumenta L1 weight\n",
    "            loss_G = 0.5 * loss_G_GAN + adaptive_lambda * loss_G_L1\n",
    "        else:\n",
    "            loss_G = loss_G_GAN + lambda_l1 * loss_G_L1\n",
    "            \n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        g_train_steps += 1\n",
    "        g_steps += 1\n",
    "\n",
    "        # Update running losses\n",
    "        running_loss_D += loss_D.item()\n",
    "        running_loss_G += loss_G.item()\n",
    "        running_loss_G_GAN += loss_G_GAN.item()\n",
    "        running_loss_G_L1 += loss_G_L1.item()\n",
    "\n",
    "        # Update progress bar\n",
    "        train_pbar.set_postfix({\n",
    "            'D_loss': f'{loss_D.item():.4f}',\n",
    "            'G_loss': f'{loss_G.item():.4f}',\n",
    "            'D_acc': f'{current_d_accuracy:.3f}',\n",
    "            'D_trained': 'Y' if should_train_d else 'N',\n",
    "            'G_GAN': f'{loss_G_GAN.item():.4f}',\n",
    "            'G_L1': f'{loss_G_L1.item():.4f}'\n",
    "        })\n",
    "\n",
    "    train_pbar.close()\n",
    "\n",
    "    # Calcola medie\n",
    "    avg_loss_D = running_loss_D / len(mixture_train_loader)\n",
    "    avg_loss_G = running_loss_G / len(mixture_train_loader)\n",
    "    avg_loss_G_GAN = running_loss_G_GAN / len(mixture_train_loader)\n",
    "    avg_loss_G_L1 = running_loss_G_L1 / len(mixture_train_loader)\n",
    "    final_d_accuracy = d_correct_predictions / max(total_predictions, 1)\n",
    "\n",
    "    # === VALIDATION ===\n",
    "    generator.eval()\n",
    "    val_loss_L1 = 0.0\n",
    "\n",
    "    val_pbar = tqdm(zip(mixture_validation_loader, cycle(background_validation_loader)),\n",
    "                    total=len(mixture_validation_loader),\n",
    "                    desc=f'Epoch {epoch+1}/{num_epochs} - Validation',\n",
    "                    leave=False, unit='batch')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mixture, background in val_pbar:\n",
    "            mixture = mixture.to(device)\n",
    "            background = background.to(device)\n",
    "            if mixture.ndim == 2:\n",
    "                mixture = mixture.unsqueeze(1)\n",
    "            if background.ndim == 2:\n",
    "                background = background.unsqueeze(1)\n",
    "\n",
    "            pred_background = generator(mixture)\n",
    "            batch_val_loss = F.l1_loss(pred_background, background).item()\n",
    "            val_loss_L1 += batch_val_loss\n",
    "\n",
    "            val_pbar.set_postfix({'Val_L1': f'{batch_val_loss:.4f}'})\n",
    "\n",
    "    val_pbar.close()\n",
    "    val_loss_L1 /= len(mixture_validation_loader)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{num_epochs}] \"\n",
    "          f\"Train_Loss_D: {avg_loss_D:.4f} | \"\n",
    "          f\"Train_Loss_G: {avg_loss_G:.4f} | \"\n",
    "          f\"G_GAN: {avg_loss_G_GAN:.4f} | \"\n",
    "          f\"G_L1: {avg_loss_G_L1:.4f} | \"\n",
    "          f\"Val_L1: {val_loss_L1:.4f} | \"\n",
    "          f\"D_Acc: {final_d_accuracy:.3f} | \"\n",
    "          f\"D_Steps: {d_train_steps} | G_Steps: {g_train_steps}\")\n",
    "\n",
    "    # === LOG E CSV ===\n",
    "    with open(csv_path, 'a', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writerow({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss_D': avg_loss_D,\n",
    "            'train_loss_G': avg_loss_G,\n",
    "            'train_loss_G_GAN': avg_loss_G_GAN,\n",
    "            'train_loss_G_L1': avg_loss_G_L1,\n",
    "            'val_loss_L1': val_loss_L1,\n",
    "            'd_accuracy': final_d_accuracy,\n",
    "            'd_train_steps': d_train_steps,\n",
    "            'g_train_steps': g_train_steps\n",
    "        })\n",
    "\n",
    "    # === CHECKPOINT ===\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "        'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'g_steps': g_steps\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "    # === BEST MODEL ===\n",
    "    if val_loss_L1 < best_val_loss:\n",
    "        best_val_loss = val_loss_L1\n",
    "        torch.save(generator.state_dict(), \"results-4/best_generator.pth\")\n",
    "        torch.save(discriminator.state_dict(), \"results-4/best_discriminator.pth\")\n",
    "        print(f\"New best model saved at epoch {epoch+1} with Val_L1: {val_loss_L1:.4f}\")\n",
    "\n",
    "    # === ADATTAMENTO DINAMICO DEI PARAMETRI ===\n",
    "    # Se il discriminatore è troppo debole (accuratezza < 60%), aumenta la sua learning rate\n",
    "    if final_d_accuracy < 0.6:\n",
    "        for param_group in optimizer_D.param_groups:\n",
    "            param_group['lr'] = min(param_group['lr'] * 1.1, 5e-4)  # Max 5e-4\n",
    "        print(f\"D accuracy too low ({final_d_accuracy:.3f}), increasing D learning rate to {optimizer_D.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    # Se il discriminatore è troppo forte (accuratezza > 85%), riduci la sua learning rate\n",
    "    elif final_d_accuracy > 0.85:\n",
    "        for param_group in optimizer_D.param_groups:\n",
    "            param_group['lr'] = max(param_group['lr'] * 0.9, 1e-5)  # Min 1e-5\n",
    "        print(f\"D accuracy too high ({final_d_accuracy:.3f}), decreasing D learning rate to {optimizer_D.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "print(f\"\\nTraining completed! Best validation L1 loss: {best_val_loss:.4f}\")\n",
    "print(f\"Training history saved to: {csv_path}\")"
   ],
   "id": "52426830e0d45f53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint...\n",
      "Resuming training from epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 - Training:   0%|          | 15/3889 [00:09<35:36,  1.81batch/s, D_loss=0.4561, G_loss=1.3446, D_acc=0.952, D_trained=N, G_GAN=1.1732, G_L1=0.0171] "
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def preprocess_audio(filepath, sample_rate=16000, channels=1):\n",
    "    audio = AudioSegment.from_file(filepath).set_frame_rate(sample_rate).set_channels(channels)\n",
    "    audio_array = np.array(audio.get_array_of_samples(), dtype=np.float32)\n",
    "    audio_array /= (1 << (8 * audio.sample_width - 1))  # normalize [-1, 1]\n",
    "    audio_tensor = torch.tensor(audio_array).unsqueeze(0).unsqueeze(0)  # shape: (1, 1, T)\n",
    "    return audio_tensor.to(device), audio.frame_rate\n",
    "\n",
    "def postprocess_and_export(tensor, filename, sample_rate=16000):\n",
    "    audio_np = tensor.squeeze().cpu().numpy()\n",
    "    audio_np = np.clip(audio_np, -1.0, 1.0)\n",
    "    audio = array_to_pydub(audio_np, sample_rate=sample_rate)\n",
    "    audio.export(filename, format=\"wav\")\n",
    "\n",
    "# Inferenza su file arbitrario\n",
    "def infer_from_path(path_to_wav, output_event_path=\"event_output.wav\"):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        mixture_tensor, sr = preprocess_audio(path_to_wav)\n",
    "        output_background = generator(mixture_tensor)\n",
    "\n",
    "        # Allinea dimensioni se serve\n",
    "        min_len = min(mixture_tensor.shape[-1], output_background.shape[-1])\n",
    "        mixture_tensor = mixture_tensor[..., :min_len]\n",
    "        output_background = output_background[..., :min_len]\n",
    "\n",
    "        # Residuo = Evento\n",
    "        estimated_event = mixture_tensor - output_background\n",
    "\n",
    "        postprocess_and_export(estimated_event, output_event_path, sample_rate=sr)\n",
    "        print(f\"Evento stimato salvato in: {output_event_path}\")\n"
   ],
   "id": "5d1797f586083c37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "infer_from_path(\"audio_sources/train_set\\mix_4962\\mixture.wav\", \"pc12_output_event.wav\")",
   "id": "ab7c003527d39e3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b9f2f180488df5e5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
